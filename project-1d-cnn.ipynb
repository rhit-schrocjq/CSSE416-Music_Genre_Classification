{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606695fc-3cad-4c2a-8407-051389cf56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f78d2a6-d833-43f9-b69d-650962b3a9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9448f-ad6d-4c33-9ba3-b22f863f1649",
   "metadata": {},
   "source": [
    "## FFT, 30-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e476fcba-f801-4202-93aa-82e1b1e30e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, root_dir, sample_rate=16000, max_length=30*16000, exclude_files=None, set_type=\"train\", train_ratio=0.2):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             root_dir (str): Root directory with genre folders containing audio files.\n",
    "#             sample_rate (int): Target sample rate for audio files.\n",
    "#             max_length (int): Maximum length of each audio sequence in samples.\n",
    "#             set_type (str): Specify \"train\" for training set and \"test\" for testing set.\n",
    "#             train_ratio (float): The ratio of the data to use for training (e.g., 0.2 for 20% train, 80% test).\n",
    "#         \"\"\"\n",
    "#         self.root_dir = root_dir\n",
    "#         self.sample_rate = sample_rate\n",
    "#         self.max_length = max_length\n",
    "#         self.exclude_files = exclude_files if exclude_files else []\n",
    "#         self.set_type = set_type\n",
    "#         self.train_ratio = train_ratio\n",
    "#         self.file_paths = []\n",
    "#         self.labels = []\n",
    "#         self.classes = sorted(os.listdir(root_dir))  # Get genre names as class labels\n",
    "\n",
    "#         file_limit = 200\n",
    "#         # Collect file paths and corresponding labels\n",
    "#         for label_idx, genre in enumerate(self.classes):\n",
    "#             genre_folder = os.path.join(root_dir, genre)\n",
    "#             i = 0\n",
    "#             for file_name in os.listdir(genre_folder):\n",
    "#                 if i == file_limit:\n",
    "#                     break\n",
    "#                 if file_name.endswith(\".wav\") and file_name not in self.exclude_files:\n",
    "#                     self.file_paths.append(os.path.join(genre_folder, file_name))\n",
    "#                     self.labels.append(label_idx)\n",
    "#                     i += 1\n",
    "\n",
    "#         # Shuffle and split data\n",
    "#         combined = list(zip(self.file_paths, self.labels))\n",
    "#         random.shuffle(combined)\n",
    "#         self.file_paths, self.labels = zip(*combined)\n",
    "\n",
    "#         # Perform train-test split\n",
    "#         split_index = int(len(self.file_paths) * self.train_ratio)\n",
    "#         if self.set_type == \"train\":\n",
    "#             # Use the first portion as the training set\n",
    "#             self.file_paths = self.file_paths[:split_index]\n",
    "#             self.labels = self.labels[:split_index]\n",
    "#         else:\n",
    "#             # Use the remaining portion as the testing set\n",
    "#             self.file_paths = self.file_paths[split_index:]\n",
    "#             self.labels = self.labels[split_index:]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths)\n",
    "    \n",
    "#     def load_audio(self, file_path):\n",
    "#         \"\"\"\n",
    "#         Load audio file using torchaudio, and fall back to librosa if needed.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             waveform, sr = torchaudio.load(file_path)\n",
    "#             if sr != self.sample_rate:\n",
    "#                 waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n",
    "#         except RuntimeError as e:\n",
    "#             print(f\"torchaudio failed for {file_path} with error: {e}. Trying librosa as fallback.\")\n",
    "#             waveform, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "#             waveform = torch.tensor(waveform).unsqueeze(0)  # Convert to PyTorch format, add channel dim\n",
    "#         return waveform\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Get an audio sample and its label from the dataset.\n",
    "\n",
    "#         Args:\n",
    "#             idx (int): Index of the audio file to retrieve.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: A tuple containing the Fourier-transformed waveform and its corresponding label.\n",
    "#         \"\"\"\n",
    "#         audio_path = self.file_paths[idx]\n",
    "        \n",
    "#         waveform = self.load_audio(audio_path)\n",
    "#         if waveform is None:\n",
    "#             print(f\"Skipping {audio_path} due to load failure.\")\n",
    "#             return None\n",
    "\n",
    "#         # Ensure mono audio\n",
    "#         if waveform.size(0) > 1:\n",
    "#             waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "#         # Pad or truncate\n",
    "#         num_samples = waveform.size(1)\n",
    "#         if num_samples > self.max_length:\n",
    "#             waveform = waveform[:, :self.max_length]\n",
    "#         else:\n",
    "#             padding = self.max_length - num_samples\n",
    "#             waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "#         # Perform Fourier Transform and keep only the first half\n",
    "#         waveform_np = waveform.numpy()  # Convert tensor to numpy array\n",
    "#         fourier_transform = np.abs(fft(waveform_np[0]))[:self.max_length // 2]  # Only the first half\n",
    "\n",
    "#         # Retrieve label\n",
    "#         label = self.labels[idx]\n",
    "\n",
    "#         return torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0), label  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15716cd-13f5-41da-afba-8de21ef40e15",
   "metadata": {},
   "source": [
    "## 10-second Clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c8315c-01a5-489b-bb6b-c53768def02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, root_dir, sample_rate=16000, segment_length=10*16000, exclude_files=None, set_type=\"train\", train_ratio=0.2):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             root_dir (str): Root directory with genre folders containing audio files.\n",
    "#             sample_rate (int): Target sample rate for audio files.\n",
    "#             segment_length (int): Length of each audio segment in samples (10 seconds).\n",
    "#             set_type (str): Specify \"train\" for training set and \"test\" for testing set.\n",
    "#             train_ratio (float): The ratio of the data to use for training (e.g., 0.2 for 20% train, 80% test).\n",
    "#         \"\"\"\n",
    "#         self.root_dir = root_dir\n",
    "#         self.sample_rate = sample_rate\n",
    "#         self.segment_length = segment_length  # Length of each segment (10 seconds)\n",
    "#         self.exclude_files = exclude_files if exclude_files else []\n",
    "#         self.set_type = set_type\n",
    "#         self.train_ratio = train_ratio\n",
    "#         self.file_paths = []\n",
    "#         self.labels = []\n",
    "#         self.classes = sorted(os.listdir(root_dir))  # Get genre names as class labels\n",
    "\n",
    "#         file_limit = 289\n",
    "#         # Collect file paths and corresponding labels\n",
    "#         for label_idx, genre in enumerate(self.classes):\n",
    "#             genre_folder = os.path.join(root_dir, genre)\n",
    "#             i = 0\n",
    "#             for file_name in os.listdir(genre_folder):\n",
    "#                 if i == file_limit:\n",
    "#                     break\n",
    "#                 if file_name.endswith(\".wav\") and file_name not in self.exclude_files:\n",
    "#                     self.file_paths.append(os.path.join(genre_folder, file_name))\n",
    "#                     self.labels.append(label_idx)\n",
    "#                     i += 1\n",
    "\n",
    "#         # Shuffle and split data\n",
    "#         combined = list(zip(self.file_paths, self.labels))\n",
    "#         random.shuffle(combined)\n",
    "#         self.file_paths, self.labels = zip(*combined)\n",
    "\n",
    "#         # Perform train-test split\n",
    "#         split_index = int(len(self.file_paths) * self.train_ratio)\n",
    "#         if self.set_type == \"train\":\n",
    "#             # Use the first portion as the training set\n",
    "#             self.file_paths = self.file_paths[:split_index]\n",
    "#             self.labels = self.labels[:split_index]\n",
    "#         else:\n",
    "#             # Use the remaining portion as the testing set\n",
    "#             self.file_paths = self.file_paths[split_index:]\n",
    "#             self.labels = self.labels[split_index:]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths) * 3  # Each file is split into 3 segments\n",
    "    \n",
    "#     def load_audio(self, file_path):\n",
    "#         \"\"\"\n",
    "#         Load audio file using torchaudio, and fall back to librosa if needed.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             waveform, sr = torchaudio.load(file_path)\n",
    "#             if sr != self.sample_rate:\n",
    "#                 waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n",
    "#         except RuntimeError as e:\n",
    "#             print(f\"torchaudio failed for {file_path} with error: {e}. Trying librosa as fallback.\")\n",
    "#             waveform, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "#             waveform = torch.tensor(waveform).unsqueeze(0)  # Convert to PyTorch format, add channel dim\n",
    "#         return waveform\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Get a 10-second audio segment and its label from the dataset.\n",
    "\n",
    "#         Args:\n",
    "#             idx (int): Index of the audio segment to retrieve.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: A tuple containing the Fourier-transformed waveform segment and its corresponding label.\n",
    "#         \"\"\"\n",
    "#         # Determine file index and segment index\n",
    "#         file_idx = idx // 3       # Index of the file in self.file_paths\n",
    "#         segment_idx = idx % 3     # Segment within the file (0, 1, or 2 for 10-second clips)\n",
    "\n",
    "#         # Load the waveform\n",
    "#         audio_path = self.file_paths[file_idx]\n",
    "#         waveform = self.load_audio(audio_path)\n",
    "#         if waveform is None:\n",
    "#             print(f\"Skipping {audio_path} due to load failure.\")\n",
    "#             return None\n",
    "\n",
    "#         # Ensure mono audio\n",
    "#         if waveform.size(0) > 1:\n",
    "#             waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "#         # Calculate start and end for each 10-second segment\n",
    "#         start = segment_idx * self.segment_length\n",
    "#         end = start + self.segment_length\n",
    "#         waveform_segment = waveform[:, start:end]\n",
    "\n",
    "#         # If segment is shorter than segment_length, pad with zeros\n",
    "#         num_samples = waveform_segment.size(1)\n",
    "#         if num_samples < self.segment_length:\n",
    "#             padding = self.segment_length - num_samples\n",
    "#             waveform_segment = torch.nn.functional.pad(waveform_segment, (0, padding))\n",
    "\n",
    "#         # Perform Fourier Transform on the segment and keep only the first half\n",
    "#         waveform_np = waveform_segment.numpy()  # Convert tensor to numpy array\n",
    "#         fourier_transform = np.abs(fft(waveform_np[0]))[:self.segment_length // 2]  # Only the first half\n",
    "\n",
    "#         # Retrieve label\n",
    "#         label = self.labels[file_idx]\n",
    "\n",
    "#         return torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0), label  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787015b5-5151-4e73-8f7a-64a9440106d7",
   "metadata": {},
   "source": [
    "# 6-second clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac3120f-3b3f-4f0a-a7ac-2400a9af09de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, root_dir, sample_rate=16000, segment_length=6*16000, exclude_files=None, set_type=\"train\", train_ratio=0.2):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             root_dir (str): Root directory with genre folders containing audio files.\n",
    "#             sample_rate (int): Target sample rate for audio files.\n",
    "#             segment_length (int): Length of each audio segment in samples (6 seconds).\n",
    "#             set_type (str): Specify \"train\" for training set and \"test\" for testing set.\n",
    "#             train_ratio (float): The ratio of the data to use for training (e.g., 0.2 for 20% train, 80% test).\n",
    "#         \"\"\"\n",
    "#         self.root_dir = root_dir\n",
    "#         self.sample_rate = sample_rate\n",
    "#         self.segment_length = segment_length  # Length of each segment (10 seconds)\n",
    "#         self.exclude_files = exclude_files if exclude_files else []\n",
    "#         self.set_type = set_type\n",
    "#         self.train_ratio = train_ratio\n",
    "#         self.file_paths = []\n",
    "#         self.labels = []\n",
    "#         self.classes = sorted(os.listdir(root_dir))  # Get genre names as class labels\n",
    "\n",
    "#         file_limit = 289\n",
    "#         # Collect file paths and corresponding labels\n",
    "#         for label_idx, genre in enumerate(self.classes):\n",
    "#             genre_folder = os.path.join(root_dir, genre)\n",
    "#             i = 0\n",
    "#             for file_name in os.listdir(genre_folder):\n",
    "#                 if i == file_limit:\n",
    "#                     break\n",
    "#                 if file_name.endswith(\".wav\") and file_name not in self.exclude_files:\n",
    "#                     self.file_paths.append(os.path.join(genre_folder, file_name))\n",
    "#                     self.labels.append(label_idx)\n",
    "#                     i += 1\n",
    "\n",
    "#         # Shuffle and split data\n",
    "#         combined = list(zip(self.file_paths, self.labels))\n",
    "#         random.shuffle(combined)\n",
    "#         self.file_paths, self.labels = zip(*combined)\n",
    "\n",
    "#         # Perform train-test split\n",
    "#         split_index = int(len(self.file_paths) * self.train_ratio)\n",
    "#         if self.set_type == \"train\":\n",
    "#             # Use the first portion as the training set\n",
    "#             self.file_paths = self.file_paths[:split_index]\n",
    "#             self.labels = self.labels[:split_index]\n",
    "#         else:\n",
    "#             # Use the remaining portion as the testing set\n",
    "#             self.file_paths = self.file_paths[split_index:]\n",
    "#             self.labels = self.labels[split_index:]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.file_paths) * 5  # Each file is split into 5 segments\n",
    "    \n",
    "#     def load_audio(self, file_path):\n",
    "#         \"\"\"\n",
    "#         Load audio file using torchaudio, and fall back to librosa if needed.\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             waveform, sr = torchaudio.load(file_path)\n",
    "#             if sr != self.sample_rate:\n",
    "#                 waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n",
    "#         except RuntimeError as e:\n",
    "#             print(f\"torchaudio failed for {file_path} with error: {e}. Trying librosa as fallback.\")\n",
    "#             waveform, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "#             waveform = torch.tensor(waveform).unsqueeze(0)  # Convert to PyTorch format, add channel dim\n",
    "#         return waveform\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Get a 5-second audio segment and its label from the dataset.\n",
    "\n",
    "#         Args:\n",
    "#             idx (int): Index of the audio segment to retrieve.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: A tuple containing the Fourier-transformed waveform segment and its corresponding label.\n",
    "#         \"\"\"\n",
    "#         # Determine file index and segment index\n",
    "#         file_idx = idx // 5       # Index of the file in self.file_paths\n",
    "#         segment_idx = idx % 5     # Segment within the file (for 6-second clips)\n",
    "\n",
    "#         # Load the waveform\n",
    "#         audio_path = self.file_paths[file_idx]\n",
    "#         waveform = self.load_audio(audio_path)\n",
    "#         if waveform is None:\n",
    "#             print(f\"Skipping {audio_path} due to load failure.\")\n",
    "#             return None\n",
    "\n",
    "#         # Ensure mono audio\n",
    "#         if waveform.size(0) > 1:\n",
    "#             waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "#         # Calculate start and end for each 5-second segment\n",
    "#         start = segment_idx * self.segment_length\n",
    "#         end = start + self.segment_length\n",
    "#         waveform_segment = waveform[:, start:end]\n",
    "\n",
    "#         # If segment is shorter than segment_length, pad with zeros\n",
    "#         num_samples = waveform_segment.size(1)\n",
    "#         if num_samples < self.segment_length:\n",
    "#             padding = self.segment_length - num_samples\n",
    "#             waveform_segment = torch.nn.functional.pad(waveform_segment, (0, padding))\n",
    "\n",
    "#         # Perform Fourier Transform on the segment and keep only the first half\n",
    "#         waveform_np = waveform_segment.numpy()  # Convert tensor to numpy array\n",
    "#         fourier_transform = np.abs(fft(waveform_np[0]))[:self.segment_length // 2]  # Only the first half\n",
    "\n",
    "#         # Retrieve label\n",
    "#         label = self.labels[file_idx]\n",
    "\n",
    "#         return torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0), label  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac832b3c-a309-40e0-9d86-455471c49023",
   "metadata": {},
   "source": [
    "## 5-second clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03221d16-03b0-4e20-8f2a-0db30103b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=16000, segment_length=5*16000, exclude_files=None, set_type=\"train\", train_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory with genre folders containing audio files.\n",
    "            sample_rate (int): Target sample rate for audio files.\n",
    "            segment_length (int): Length of each audio segment in samples (5 seconds).\n",
    "            set_type (str): Specify \"train\" for training set and \"test\" for testing set.\n",
    "            train_ratio (float): The ratio of the data to use for training (e.g., 0.2 for 20% train, 80% test).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length = segment_length  # Length of each segment (5 seconds)\n",
    "        self.exclude_files = exclude_files if exclude_files else []\n",
    "        self.set_type = set_type\n",
    "        self.train_ratio = train_ratio\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))  # Get genre names as class labels\n",
    "\n",
    "        file_limit = 289\n",
    "        # Collect file paths and corresponding labels\n",
    "        for label_idx, genre in enumerate(self.classes):\n",
    "            genre_folder = os.path.join(root_dir, genre)\n",
    "            i = 0\n",
    "            for file_name in os.listdir(genre_folder):\n",
    "                if i == file_limit:\n",
    "                    break\n",
    "                if file_name.endswith(\".wav\") and file_name not in self.exclude_files:\n",
    "                    self.file_paths.append(os.path.join(genre_folder, file_name))\n",
    "                    self.labels.append(label_idx)\n",
    "                    i += 1\n",
    "\n",
    "        # Shuffle and split data\n",
    "        combined = list(zip(self.file_paths, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.file_paths, self.labels = zip(*combined)\n",
    "\n",
    "        # Perform train-test split\n",
    "        split_index = int(len(self.file_paths) * self.train_ratio)\n",
    "        if self.set_type == \"train\":\n",
    "            # Use the first portion as the training set\n",
    "            self.file_paths = self.file_paths[:split_index]\n",
    "            self.labels = self.labels[:split_index]\n",
    "        else:\n",
    "            # Use the remaining portion as the testing set\n",
    "            self.file_paths = self.file_paths[split_index:]\n",
    "            self.labels = self.labels[split_index:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths) * 6  # Each file is split into 6 segments\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"\n",
    "        Load audio file using torchaudio\n",
    "        \"\"\"\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a 5-second audio segment and its label from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the audio segment to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the Fourier-transformed waveform segment and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Determine file index and segment index\n",
    "        file_idx = idx // 6       # Index of the file in self.file_paths\n",
    "        segment_idx = idx % 6     # Segment within the file (for 5-second clips)\n",
    "\n",
    "        # Load the waveform\n",
    "        audio_path = self.file_paths[file_idx]\n",
    "        waveform = self.load_audio(audio_path)\n",
    "        if waveform is None:\n",
    "            print(f\"Skipping {audio_path} due to load failure.\")\n",
    "            return None\n",
    "\n",
    "        # Ensure mono audio\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Calculate start and end for each 5-second segment\n",
    "        start = segment_idx * self.segment_length\n",
    "        end = start + self.segment_length\n",
    "        waveform_segment = waveform[:, start:end]\n",
    "\n",
    "        # If segment is shorter than segment_length, pad with zeros\n",
    "        num_samples = waveform_segment.size(1)\n",
    "        if num_samples < self.segment_length:\n",
    "            padding = self.segment_length - num_samples\n",
    "            waveform_segment = torch.nn.functional.pad(waveform_segment, (0, padding))\n",
    "\n",
    "        # Perform Fourier Transform on the segment and keep only the first half\n",
    "        waveform_np = waveform_segment.numpy()  # Convert tensor to numpy array\n",
    "        fourier_transform = np.abs(fft(waveform_np[0]))[:self.segment_length // 2]  # Only the first half\n",
    "\n",
    "        # Retrieve label\n",
    "        label = self.labels[file_idx]\n",
    "\n",
    "        return torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0), label  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93b59e-f67a-40b3-92a6-27962dc8fa48",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e7d89e-a2c3-48b5-bd1b-1372bedcf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Define simple and lame 1D CNN model for audio classification\n",
    "# # # add a second fully connected layer\n",
    "\n",
    "# class AudioCNN(nn.Module):\n",
    "#     def __init__(self, input_length=30*16000):\n",
    "#         super(AudioCNN, self).__init__()\n",
    "        \n",
    "#         # Define convolutional layers\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "#         # Define pooling layer\n",
    "#         self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Calculate the flattened size after the last pooling layer\n",
    "#         # Pass a dummy input through the conv and pool layers to find the shape\n",
    "#         with torch.no_grad():\n",
    "#             dummy_input = torch.zeros(1, 1, input_length)  # Batch size 1, 1 channel, input length\n",
    "#             out = self.pool(F.relu(self.conv1(dummy_input)))\n",
    "#             out = self.pool(F.relu(self.conv2(out)))\n",
    "#             out = self.pool(F.relu(self.conv3(out)))\n",
    "#             self.flattened_size = out.shape[1] * out.shape[2]  # Channels * Width after conv/pooling layers\n",
    "\n",
    "#         # Define fully connected layers using the computed flattened size\n",
    "#         self.fc1 = nn.Linear(self.flattened_size, 128)  # Match flattened size here\n",
    "#         self.fc2 = nn.Linear(128, 32)  # Match flattened size here\n",
    "#         self.fc3 = nn.Linear(32, 10)  # Example output layer, e.g., for 10 classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Pass through convolutional and pooling layers\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "#         # Flatten the tensor to match the input of the fully connected layer\n",
    "#         x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))  # Pass through first fully connected layer\n",
    "#         x = F.relu(self.fc2(x))  # Pass through second fully connected layer\n",
    "#         x = self.fc3(x)          # Output layer\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# # Accuracy of the model on the test set (1s clips, 1 fc layer): 31.54%\n",
    "# # Accuracy of the model on the test set (30s clips, 1 fc layer): 33.26% (3.4507, 2.0044, 1.3442, 0.6947, 0.2494)\n",
    "# # Accuracy of the model on the test set (100 30s clips, 2 fc layer): 16.12% (15.5413, 4.4464, 2.5460, 2.0580, 1.7405)\n",
    "# # Accuracy of the model on the test set (200 30s clips, 2 fc layer): 30.06% (5.9436, 2.3099, 2.2481, 1.6181, 0.8132)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01085c3e-7bf8-4fcc-86ab-28cc9e377c6e",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2320d6e7-b3ca-47e9-823c-9493c5d9b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a 1D CNN model for audio classification that imitates the 2D\n",
    "# # give up on this one because of memory issues\n",
    "# class AudioCNN(nn.Module):\n",
    "#     def __init__(self, input_length=30*16000):\n",
    "#         super(AudioCNN, self).__init__()\n",
    "        \n",
    "#         # Define convolutional layers\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv2 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "\n",
    "#         # Define pooling layer\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Define convolutional layers\n",
    "#         self.conv3 = nn.Conv1d(in_channels=8, out_channels=12, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv4 = nn.Conv1d(in_channels=12, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "\n",
    "#         # Define subsampling layer\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # # Define convolutional layers\n",
    "#         # self.conv5 = nn.Conv1d(in_channels=16, out_channels=20, kernel_size=3, stride=1, padding='same')\n",
    "#         # self.conv6 = nn.Conv1d(in_channels=20, out_channels=24, kernel_size=3, stride=1, padding='same')\n",
    "\n",
    "#         # # Define subsampling layer\n",
    "#         # self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Calculate the flattened size after the last pooling layer\n",
    "#         # Pass a dummy input through the conv and pool layers to find the shape\n",
    "#         with torch.no_grad():\n",
    "#             dummy_input = torch.zeros(1, 1, input_length)  # Batch size 1, 1 channel, input length\n",
    "#             out = F.relu(self.conv1(dummy_input))\n",
    "#             out = F.relu(self.conv2(out))\n",
    "#             out = self.pool1(out)  # Apply pool1 after conv2\n",
    "#             out = F.relu(self.conv3(out))\n",
    "#             out = F.relu(self.conv4(out))\n",
    "#             out = self.pool2(out)  # Apply pool2 after conv4\n",
    "#             # out = F.relu(self.conv5(out))\n",
    "#             # out = F.relu(self.conv6(out))\n",
    "#             # out = self.pool3(out)  # Apply pool3 after conv6\n",
    "#             self.flattened_size = out.shape[1] * out.shape[2]  # Channels * Width after conv/pooling layers\n",
    "\n",
    "        \n",
    "#         # Define fully connected layers using the computed flattened size\n",
    "#         self.fc1 = nn.Linear(in_features=self.flattened_size, out_features=5000)\n",
    "#         # self.fc2 = nn.Linear(in_features=5000, out_features=1000)\n",
    "#         # self.fc3 = nn.Linear(in_features=1000, out_features=100)\n",
    "#         self.fc4 = nn.Linear(in_features=5000, out_features=10)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Pass through convolutional and pooling layers\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool1(x)\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         x = self.pool2(x)\n",
    "#         # x = F.relu(self.conv5(x))\n",
    "#         # x = F.relu(self.conv6(x))\n",
    "#         # x = self.pool3(x)\n",
    "\n",
    "#         # Flatten the tensor to match the input of the fully connected layer\n",
    "#         x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))  # Pass through first fully connected layer\n",
    "#         # x = F.relu(self.fc2(x))  # Pass through first fully connected layer\n",
    "#         # x = F.relu(self.fc3(x))  # Pass through first fully connected layer\n",
    "#         x = self.fc4(x)          # Output layer\n",
    "#         return x\n",
    "\n",
    "# # Accuracy of the model on the test set (30s clips): 10.07%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7333a8-f760-41e5-b00c-78de51a46cfe",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35621c2-20c3-465b-9553-afcf551ca5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a 1D CNN model for Fourier-transformed audio data with multiple input channels\n",
    "# class AudioCNN(nn.Module):\n",
    "#     def __init__(self, input_length=30*16000 // 2):\n",
    "#         super(AudioCNN, self).__init__()\n",
    "        \n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding='same')\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv2 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Calculate the flattened size after the last pooling layer\n",
    "#         with torch.no_grad():\n",
    "#             dummy_input = torch.zeros(1, 1, input_length)  # Batch size 1, 1 channel, input length\n",
    "#             out = F.relu(self.conv1(dummy_input))\n",
    "#             out = self.pool1(out)\n",
    "#             out = F.relu(self.conv2(out))\n",
    "#             out = self.pool2(out) \n",
    "#             self.flattened_size = out.shape[1] * out.shape[2]  # Channels * Width after conv/pooling layers\n",
    "\n",
    "#         # Define fully connected layers using the computed flattened size\n",
    "#         self.fc1 = nn.Linear(in_features=self.flattened_size, out_features=1000)\n",
    "#         self.fc2 = nn.Linear(in_features=1000, out_features=10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool1(x)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool2(x)\n",
    "\n",
    "#         x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Accuracy of the model on the test set (30 s, fft, batch32): 42.94% (626.5556, 51.8651, 3.5706)\n",
    "# # Accuracy of the model on the test set (30, fft, batch32): 44.25% (60.5790, 4.5441, 2.2963, 0.5291, 0.0785)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b0e7d-97dd-4425-a1be-38be0e1c8381",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77bc076e-4350-497f-ae1b-ddaa39f6f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another 1D CNN model for Fourier-transformed audio data\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, input_length=5*16000 // 2):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer and pooling\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolutional layer and pooling\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolutional layer and pooling\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fourth convolutional layer and pooling\n",
    "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fifth convolutional layer and pooling\n",
    "        self.conv5 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool5 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flattened size after the last pooling layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_length)  # Batch size 1, 1 channel, input length\n",
    "            out = F.relu(self.conv1(dummy_input))\n",
    "            out = self.pool1(out)\n",
    "            out = F.relu(self.conv2(out))\n",
    "            out = self.pool2(out)\n",
    "            out = F.relu(self.conv3(out))\n",
    "            out = self.pool3(out)\n",
    "            out = F.relu(self.conv4(out))\n",
    "            out = self.pool4(out)\n",
    "            out = F.relu(self.conv5(out))\n",
    "            out = self.pool5(out)\n",
    "            self.flattened_size = out.shape[1] * out.shape[2]  # Channels * Width after conv/pooling layers\n",
    "\n",
    "        # Define fully connected layers using the computed flattened size\n",
    "        self.fc1 = nn.Linear(in_features=self.flattened_size, out_features=3200)\n",
    "        self.fc2 = nn.Linear(in_features=3200, out_features=1600)\n",
    "        self.fc3 = nn.Linear(in_features=1600, out_features=800)\n",
    "        self.fc4 = nn.Linear(in_features=800, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional and pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor to match the input of the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))    # Pass through first fully connected layer\n",
    "        x = F.relu(self.fc2(x))    # Pass through second fully connected layer\n",
    "        x = F.relu(self.fc3(x))    # Pass through third fully connected layer\n",
    "        x = self.fc4(x)            # Output layer\n",
    "        return x\n",
    "\n",
    "# Accuracy of the model on the test set (30s, 2cp+2fc): 49.94% (259.7404, 43.7874, 2.2371, 0.4296, 0.0983)\n",
    "# Accuracy of the model on the test set (30s, 3cp+2fc): 46.44% (665.3874, 170.0507, 69.5730, 25.9738, 4.6241)\n",
    "# Accuracy of the model on the test set (30s, 10epoch, 2cp+2fc): 51.38% (8451.4932, 969.9626, 234.5838, 81.7483, 4.4840, 6.5689, 0.9066, 2.7939, 0.7037, 1.3528)\n",
    "# Accuracy of the model on the test set (10s, 10epoch, 2cp+2fc): 51.47% (28.3695, 0.4100, 0.5335, 0.2201, 0.1588, 0.2497, 3.6753, 0.0198, 0.0388, 4.8844)\n",
    "# Accuracy of the model on the test set (10s, 10epoch, 3cp+2fc): 57.58% (2.2222, 1.1562, 0.2161, 0.1510, 0.0169, 0.0003, 0.0004, 0.0000, 0.0002, 0.0001)\n",
    "# Accuracy of the model on the test set (10s, 10epoch, 3cp+3fc): 59.18% (2.4068, 0.8191, 0.4002, 0.4372, 0.1581, 0.0804, 0.0336, 0.0566, 0.1198, 0.0088)\n",
    "# Accuracy of the model on the test set (6s, 10epoch, 3cp+3fc): 57.18% (1.3956, 0.6558, 0.0348, 0.0102, 0.0113, 0.0274, 0.0019, 0.0001, 0.0105, 0.0013)\n",
    "# Accuracy of the model on the test set (6s, 10epoch, 4cp+3fc): 57.89% (1.1886, 1.2358, 0.1747, 0.5024, 0.0492, 0.1019, 0.0037, 0.0001, 0.0006, 0.000057)\n",
    "# Accuracy of the model on the test set (6s, 10epoch, 4cp+4fc): 49.52% (1.6578, 1.4446, 0.8011, 0.1661, 0.2279, 0.0614, 0.0247, 0.0033, 0.0036, 0.0100)\n",
    "# Accuracy of the model on the test set (6s, 10epoch, 5cp+3fc): 54.86% (1.2255, 1.7275, 0.3796, 0.1867, 0.0577, 0.0010, 0.0031, 0.0214, 0.0040, 0.0087)\n",
    "# Accuracy of the model on the test set (5s, 10epoch, 5cp+3fc): 59.83% (1.1961, 0.8041, 0.6351, 0.0491, 0.1683, 0.0060, 0.0005, 0.0004, 0.0001, 0.0003)\n",
    "# Accuracy of the model on the test set (5s, 10epoch, 5cp+4fc): 55.64% (1.4656, 0.9191, 0.9812, 0.0128, 0.0013, 0.0102, 0.0771, 0.0010, 0.0003, 0.0011)\n",
    "# Accuracy of the model on the test set (5s, 10epoch, 5cp+4fc, bigger fc): 58.55% (1.8197, 1.1054, 0.4446, 0.0244, 0.0141, 0.0133, 0.0765, 0.0001, 0.0003, 0.0001)\n",
    "# Accuracy of the model on the test set (5s, 10epoch, 4cp+4fc, bigger cp): 56.18% (1.2513, 0.8952, 0.2364, 0.1094, 0.1831, 0.0042, 0.0216, 0.3237, 0.0035, 0.0020)\n",
    "# Accuracy of the model on the test set (5s, 7epoch, 5cp+4fc): 55.83% (1.4570, 1.4312, 0.1253, 0.0622, 0.4947, 0.2065, 0.0038)\n",
    "# Accuracy of the model on the test set (5s, 10epoch, 5cp+4fc, bigger fc) : 56.87% (1.5426, 0.9646, 0.2376, 0.2390, 0.0408, 0.0038, 0.0624, 0.0001, 0.0002, 0.0001)\n",
    "# Accuracy of the model on the test set (5s, 10epoch, 5cp+4fc, 20filesfromeachgenre): 47.40%\n",
    "# Accuracy of the model on the test set: 57.54%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6d939-d112-4cac-8b01-25b41ac8283d",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e158603-95a5-4a24-aa9e-254b4ffa0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define another 1D CNN model for Fourier-transformed audio data\n",
    "# class AudioCNN(nn.Module):\n",
    "#     def __init__(self, input_length=5*16000 // 2):\n",
    "#         super(AudioCNN, self).__init__()\n",
    "        \n",
    "#         # First convolutional layer and pooling\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv2 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Second convolutional layer and pooling\n",
    "#         self.conv3 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv4 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Third convolutional layer and pooling\n",
    "#         self.conv5 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "#         self.conv6 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n",
    "#         self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         # Calculate the flattened size after the last pooling layer\n",
    "#         with torch.no_grad():\n",
    "#             dummy_input = torch.zeros(1, 1, input_length)  # Batch size 1, 1 channel, input length\n",
    "            \n",
    "#             out = F.relu(self.conv1(dummy_input))\n",
    "#             out = F.relu(self.conv2(out))\n",
    "#             out = self.pool1(out)\n",
    "            \n",
    "#             out = F.relu(self.conv3(out))\n",
    "#             out = F.relu(self.conv4(out))\n",
    "#             out = self.pool2(out)\n",
    "\n",
    "#             out = F.relu(self.conv5(out))\n",
    "#             out = F.relu(self.conv6(out))\n",
    "#             out = self.pool3(out)\n",
    "            \n",
    "#             self.flattened_size = out.shape[1] * out.shape[2]  # Channels * Width after conv/pooling layers\n",
    "\n",
    "#         # Define fully connected layers using the computed flattened size\n",
    "#         self.fc1 = nn.Linear(in_features=self.flattened_size, out_features=2000)\n",
    "#         self.fc2 = nn.Linear(in_features=2000, out_features=10)\n",
    "#         # self.fc3 = nn.Linear(in_features=2400, out_features=800)\n",
    "#         # self.fc4 = nn.Linear(in_features=800, out_features=10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Pass through convolutional and pooling layers\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool1(x)\n",
    "\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         x = self.pool2(x)\n",
    "\n",
    "#         x = F.relu(self.conv5(x))\n",
    "#         x = F.relu(self.conv6(x))\n",
    "#         x = self.pool3(x)\n",
    "        \n",
    "#         # Flatten the tensor to match the input of the fully connected layer\n",
    "#         x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))    # Pass through first fully connected layer\n",
    "#         # x = F.relu(self.fc2(x))    # Pass through second fully connected layer\n",
    "#         # x = F.relu(self.fc3(x))    # Pass through third fully connected layer\n",
    "#         x = self.fc2(x)            # Output layer\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f816e10-4fed-4100-9d1a-74e01757731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory for the data\n",
    "root_dir = \"genres_original\"\n",
    "\n",
    "# 1. Create training and testing datasets\n",
    "train_dataset = AudioDataset(root_dir=root_dir, set_type=\"train\", exclude_files=['jazz.00054.wav'])\n",
    "test_dataset = AudioDataset(root_dir=root_dir, set_type=\"test\", exclude_files=['jazz.00054.wav'])\n",
    "\n",
    "# 2. Create a DataLoader only for the training dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Optionally, a DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ccb8a28-6fba-42c8-99fb-bf40c9ef7cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioCNN(\n",
      "  (conv1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=160000, out_features=3200, bias=True)\n",
      "  (fc2): Linear(in_features=3200, out_features=1600, bias=True)\n",
      "  (fc3): Linear(in_features=1600, out_features=800, bias=True)\n",
      "  (fc4): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model, define loss function and optimizer\n",
    "model = AudioCNN()\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ecfc13b-b40b-463f-a4e0-273fbb659770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.8042\n",
      "Epoch [2/10], Loss: 0.4773\n",
      "Epoch [3/10], Loss: 0.1926\n",
      "Epoch [4/10], Loss: 0.0476\n",
      "Epoch [5/10], Loss: 0.2593\n",
      "Epoch [6/10], Loss: 0.0005\n",
      "Epoch [7/10], Loss: 0.0021\n",
      "Epoch [8/10], Loss: 0.0003\n",
      "Epoch [9/10], Loss: 0.0051\n",
      "Epoch [10/10], Loss: 0.0006\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# 3. Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:  # Only using train_loader for training loop\n",
    "        waveforms, labels = batch\n",
    "        waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(waveforms)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1731c9a-d38f-4587-bed0-0fe877d41a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"MusicGenreClassifier1D_M4.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e3d22f-04f9-4131-9a13-863aa132ffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 57.54%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop to calculate accuracy\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for waveforms, labels in test_loader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(waveforms)\n",
    "\n",
    "            # Get the predicted class (index of max log-probability)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy\n",
    "    return accuracy\n",
    "\n",
    "# Assuming you have already created test_loader, and trained your model\n",
    "accuracy = evaluate_model(model, test_loader, device)\n",
    "print(f'Accuracy of the model on the test set: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e7b02-51da-4828-b2c8-b50e96c80dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d11c3f-5e28-4c1d-abce-15c7702a86df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdbb78c-f891-4816-904c-3797bcae4572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
