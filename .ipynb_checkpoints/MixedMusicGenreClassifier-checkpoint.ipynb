{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25639d23-11cc-4d26-a91b-fba5ecf3e11a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from panns_inference import AudioTagging\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d297645-a6a9-4c65-956d-cc3101b53ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"genres_original\"\n",
    "genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "train_save_path = \"train_genres_images\"\n",
    "test_save_path = \"test_genres_images\"\n",
    "num_divisions = 5\n",
    "max_per_genre = 350\n",
    "clip_length = 0\n",
    "train_per_test = 10\n",
    "\n",
    "if (False):\n",
    "    for genre in genres:\n",
    "        files = [f for f in os.listdir(path + \"/\" + genre) if os.path.isfile(os.path.join(path + \"/\" + genre, f))]\n",
    "        print(\"Generating \" + genre)\n",
    "        os.makedirs(os.path.dirname(train_save_path + \"/\" + genre + \"/\"), exist_ok=True) # make path if doesn't exist\n",
    "        os.makedirs(os.path.dirname(test_save_path + \"/\" + genre + \"/\"), exist_ok=True) # make path if doesn't exist\n",
    "        \n",
    "        n=0\n",
    "        for file in files:\n",
    "            if(n>=max_per_genre):\n",
    "                break\n",
    "                \n",
    "            if(file == \"jazz.00054.wav\"):\n",
    "                continue\n",
    "            else:\n",
    "                sample_rate, samples = wav.read(path + \"/\" + genre + \"/\" + file) # extract audio)\n",
    "                y, sr = librosa.load(path + \"/\" + genre + \"/\" + file, sr=32000, mono=True) #addition for instrument detection----------------------\n",
    "                    \n",
    "            # convert to mono\n",
    "            if len(samples.shape) > 1:\n",
    "                # Do a mean of all channels and keep it in one channel\n",
    "                samples = np.mean(samples, axis=1)\n",
    "                \n",
    "    \n",
    "            # pad so divisible by num_divisions\n",
    "            samples = np.append(samples, np.zeros(num_divisions - (len(samples) % num_divisions)))\n",
    "    \n",
    "            # perform STFT on 6sec samples\n",
    "            clip_samples = np.split(samples, num_divisions, axis=0)\n",
    "            clip_length = len(clip_samples[0])\n",
    "            start = clip_length // 2\n",
    "            end = start + clip_length\n",
    "            \n",
    "            while (end < len(samples)):\n",
    "                clip_samples.append(samples[start: end])\n",
    "                start = end\n",
    "                end = end + clip_length\n",
    "\n",
    "            #addition for instrument detection---------------------------------------------------------------------------------------\n",
    "            # pad so divisible by num_divisions\n",
    "            y = np.append(y, np.zeros(num_divisions - (len(y) % num_divisions)))\n",
    "            \n",
    "            instrument_clip_samples = np.split(y, num_divisions, axis=0)\n",
    "            instrument_clip_length = len(instrument_clip_samples[0])\n",
    "            start = instrument_clip_length // 2\n",
    "            end = start + instrument_clip_length\n",
    "            \n",
    "            while (end < len(y)):\n",
    "                instrument_clip_samples.append(y[start: end])\n",
    "                start = end\n",
    "                end = end + instrument_clip_length\n",
    "    \n",
    "            i=0\n",
    "            for sample in clip_samples:\n",
    "                SFT = signal.ShortTimeFFT.from_window(win_param='tukey', \n",
    "                                                      fs=sample_rate, \n",
    "                                                      nperseg=sample_rate//20,      #make 20Hz minimum sampled frequency\n",
    "                                                      noverlap=(sample_rate//20)//2,  #50% overlap\n",
    "                                                      fft_mode='onesided', \n",
    "                                                      scale_to='magnitude', \n",
    "                                                      phase_shift=None,\n",
    "                                                      symmetric_win=True)\n",
    "                Zxx = SFT.stft(sample)\n",
    "                t = SFT.t(len(sample))\n",
    "                f = SFT.f\n",
    "                Ix = instrument_clip_samples[i]\n",
    "                \n",
    "                if(n%train_per_test == 0):\n",
    "                    np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_I.npy\", Ix) #addition for instrument detection\n",
    "                    np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_Z.npy\", Zxx)\n",
    "                    #np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_T.npy\", t)\n",
    "                    #np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_F.npy\", f)\n",
    "                else:\n",
    "                    np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_I.npy\", Ix) #addition for instrument detection\n",
    "                    np.save(train_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_Z.npy\", Zxx)\n",
    "                    #np.save(train_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_T.npy\", t)\n",
    "                    #np.save(train_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_F.npy\", f)\n",
    "                    \n",
    "                i+=1\n",
    "            n+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6151b60c-7c55-400a-86b8-36d5aa73e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "#device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf52fcc-2ed3-402c-8af2-de4ae0c87734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.Zsamples = []\n",
    "        self.Tsamples = []\n",
    "        self.Fsamples = []\n",
    "        \n",
    "        # Load file paths and labels\n",
    "        for label, class_dir in enumerate(os.listdir(root_dir)):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\"Z.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Zsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"T.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Tsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"F.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Fsamples.append((file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Zsamples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.Zsamples[idx]\n",
    "        data = np.load(file_path)  # Load numpy array\n",
    "        real = np.real(data)\n",
    "        imag = np.imag(data)\n",
    "        mag = np.abs(data)\n",
    "        angle = np.angle(data)\n",
    "        real = np.expand_dims(real, axis=0)\n",
    "        imag = np.expand_dims(imag, axis=0)\n",
    "        mag = np.expand_dims(mag, axis=0)\n",
    "        angle = np.expand_dims(angle, axis=0)\n",
    "        data = np.concatenate((real, imag, mag, angle), axis=0)\n",
    "        data = torch.tensor(data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def spectrograms(self, idx):\n",
    "        Zfile_path, label = self.Zsamples[idx]\n",
    "        Tfile_path, label = self.Tsamples[idx]\n",
    "        Ffile_path, label = self.Fsamples[idx]\n",
    "        Zdata = np.load(Zfile_path)  # Load numpy array\n",
    "        Tdata = np.load(Tfile_path)  # Load numpy array\n",
    "        Fdata = np.load(Ffile_path)  # Load numpy array\n",
    "        print(Zdata.shape)\n",
    "        # Create a 2x2 subplot grid\n",
    "        fig, axes = plt.subplots(5, 1, figsize=(10, 16))\n",
    "        \n",
    "        # First subplot\n",
    "        c1 = axes[0].pcolormesh(Tdata, Fdata, np.log(np.abs(Zdata)), cmap='gnuplot')\n",
    "        fig.colorbar(c1, ax=axes[0])\n",
    "        axes[0].set_title(\"Spectrogram Magnitude\")\n",
    "        \n",
    "        # Second subplot\n",
    "        c2 = axes[1].pcolormesh(Tdata, Fdata, np.angle(Zdata), cmap='gnuplot')\n",
    "        fig.colorbar(c2, ax=axes[1])\n",
    "        axes[1].set_title(\"Spectrogram Angle\")      \n",
    "\n",
    "        # Third subplot\n",
    "        c3 = axes[2].pcolormesh(Tdata, Fdata, np.log(np.square(np.real(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c3, ax=axes[2])\n",
    "        axes[2].set_title(\"Spectrogram Real\")  \n",
    "\n",
    "        # Fourth subplot\n",
    "        c4 = axes[3].pcolormesh(Tdata, Fdata, np.log(np.square(np.imag(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c4, ax=axes[3])\n",
    "        axes[3].set_title(\"Spectrogram Imag\")  \n",
    "\n",
    "        # Fifth subplot\n",
    "        c5 = axes[4].pcolormesh(Tdata, Fdata, np.log(np.square(np.imag(Zdata)) + np.square(np.real(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c4, ax=axes[4])\n",
    "        axes[4].set_title(\"Spectrogram Imag + Real\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7787eaca-6e39-4394-8b86-56d9f2740a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "# - ToTensor: Converts the image to a PyTorch tensor\n",
    "# - Normalize: Normalizes using mean and std of the ImageNet dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((544, 240)),\n",
    "    transforms.Normalize((0,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022b9ff1-b041-4a15-ae9b-b97bbeb3b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24120\n",
      "2691\n"
     ]
    }
   ],
   "source": [
    "train_root_dir = \"train_genres_images/\"\n",
    "test_root_dir = \"test_genres_images/\"\n",
    "\n",
    "train_dataset = NumpyDataset(root_dir=train_root_dir, transform=transform)\n",
    "test_dataset = NumpyDataset(root_dir=test_root_dir, transform=transform)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe07567-bb56-48f7-85ca-e9e1fbef4c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Datasets and DataLoaders for training and testing\n",
    "# Define the root directory where data is stored\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print('Data loaders created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ecd13a-2e0d-43ce-a9b1-916d9a8c2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MusicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MusicNet, self).__init__()\n",
    "\n",
    "        self.cLayer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=64, kernel_size=3, stride=1, padding='same', groups=4),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fLayer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*34*15, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = self.cLayer1(x)\n",
    "        x = self.cLayer2(x)\n",
    "        x = self.cLayer3(x)\n",
    "        x = self.cLayer4(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], 512*34*15)       # Flatten before passing to fully connected layers\n",
    "\n",
    "        if not return_features:\n",
    "            x = self.fLayer(x)\n",
    "             \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1efe121-6297-4d50-b4da-3d7c594eda30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicNet(\n",
      "  (cLayer1): Sequential(\n",
      "    (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=4)\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fLayer): Sequential(\n",
      "    (0): Linear(in_features=261120, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_path = \"MusicGenreClassifier.pth\"\n",
    "model = MusicNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb35a8e-00a2-46db-9094-f624e3348ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store features and labels\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a393f10-1aa2-45f9-8057-a346ad05a85f",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.08 MiB for an array with shape (4, 552, 242) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# We don't need gradients for this task\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      4\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Extract features from the images\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# The VGG16 model outputs feature maps, which we can treat as image features\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Rose-Hulman Institute of Technology\\Documents\\CSSE416\\my_environment\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\OneDrive - Rose-Hulman Institute of Technology\\Documents\\CSSE416\\my_environment\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\OneDrive - Rose-Hulman Institute of Technology\\Documents\\CSSE416\\my_environment\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m, in \u001b[0;36mNumpyDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     37\u001b[0m mag \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(mag, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     38\u001b[0m angle \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(angle, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((real, imag, mag, angle), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     40\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Convert to PyTorch tensor\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.08 MiB for an array with shape (4, 552, 242) and data type float64"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "with torch.no_grad():  # We don't need gradients for this task\n",
    "    for images, label in train_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Extract features from the images\n",
    "        # The VGG16 model outputs feature maps, which we can treat as image features\n",
    "        outputs = model(images, return_features=True)\n",
    "        \n",
    "        # Flatten the outputs to a 1D vector\n",
    "        outputs = outputs.view(outputs.size(0), -1)\n",
    "        \n",
    "        # Append features and labels to lists\n",
    "        features.append(outputs.cpu().numpy())\n",
    "        labels.append(label.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b392eb-08f8-4afe-842b-3d733e13ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to arrays\n",
    "features = np.concatenate(features, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c688af-d831-4ac4-ab3f-5c5e0e24fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted features and labels\n",
    "# np.savez is used to save multiple arrays into a single file\n",
    "np.savez('music_features.npz', features=features, labels=labels)\n",
    "\n",
    "print(\"Features and labels saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd34911-09ca-4c1f-a6a2-36f8a39f9e4e",
   "metadata": {},
   "source": [
    "# BEGIN INTRUMENT feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7434f8-401b-47a2-97f2-4b1c5ca49647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VGGish model and AudioSet classifier\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "audio_tagging = AudioTagging(checkpoint_path=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c566fc-6fb0-4c93-9fd5-fb18d43ea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class InstrumentDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.Isamples = []\n",
    "        \n",
    "        # Load file paths and labels\n",
    "        for label, class_dir in enumerate(os.listdir(root_dir)):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\"I.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Zsamples.append((file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Isamples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.Isamples[idx]\n",
    "        data = np.load(file_path)  # Load numpy array\n",
    "        data = torch.tensor(data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2dcf31-0617-45bb-bc54-219b5b54f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root_dir = \"train_genres_images/\"\n",
    "test_root_dir = \"test_genres_images/\"\n",
    "\n",
    "train_dataset = NumpyDataset(root_dir=train_root_dir)\n",
    "test_dataset = NumpyDataset(root_dir=test_root_dir)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428f7d3-3a9f-4610-b4ae-c98076e94b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2c016-5c74-468c-974b-42976ab98204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store features and labels\n",
    "features = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e4853-43e7-4b18-ac41-b06709653ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "for audio, label in train_loader:\n",
    "    audio = audio.to(device)\n",
    "    \n",
    "    # Extract features for the batch\n",
    "    _, embedding = audio_tagging.inference(audio)\n",
    "\n",
    "    # Flatten the outputs to a 1D vector\n",
    "    outputs = embedding.view(embedding.size(0), -1)\n",
    "    \n",
    "    # Append features and labels to lists\n",
    "    features.append(outputs.cpu().numpy())\n",
    "    labels.append(label.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a7055-00f3-452c-a074-bbf7ab3f3f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to arrays\n",
    "features = np.concatenate(features, axis=0)\n",
    "labels = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20bb0e6-16bf-4abd-9c64-2d9ee489e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted features and labels\n",
    "# np.savez is used to save multiple arrays into a single file\n",
    "np.savez('instrument_features.npz', features=features, labels=labels)\n",
    "\n",
    "print(\"Features and labels saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f83931-cada-4704-a27c-1bba94ebdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5, patience=5):\n",
    "    time_start = time()\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() \n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss /= len(test_loader)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_loss:.4f}', Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "\n",
    "    time_stop = time()\n",
    "    time_elapsed = time_stop - time_start\n",
    "    print(f'elapsed time {round(time_elapsed,1)} sec.')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=100, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd21685-772f-410d-a0a8-db48feb3f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"MixedMusicGenreClassifier.pth\"\n",
    "#torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32508471-0922-4881-add3-39f4653c35ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
