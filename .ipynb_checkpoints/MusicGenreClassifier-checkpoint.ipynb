{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfd83d3-3104-470f-9043-70fd8ed374e1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d3d5cc-4551-4f35-9689-48610f025e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc463c-6b29-4695-bf27-aa1556a37d0f",
   "metadata": {},
   "source": [
    "### Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d813e8b1-a22e-4615-851e-3cb617f1076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"genres_original\"\n",
    "genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "save_path = \"genres_images\"\n",
    "max_per_genre = 280\n",
    "\n",
    "if (False):\n",
    "    for genre in genres:\n",
    "        files = [f for f in os.listdir(path + \"/\" + genre) if os.path.isfile(os.path.join(path + \"/\" + genre, f))]\n",
    "        print(\"Generating \" + genre)\n",
    "        n=0\n",
    "        for file in files:\n",
    "            if(n>=max_per_genre):\n",
    "                break\n",
    "                \n",
    "            if(file == \"jazz.00054.wav\"):\n",
    "                continue\n",
    "            else:\n",
    "                sample_rate, samples = wav.read(path + \"/\" + genre + \"/\" + file) # extract audio)\n",
    "                    \n",
    "            # convert to mono\n",
    "            if len(samples.shape) > 1:\n",
    "                # Do a mean of all channels and keep it in one channel\n",
    "                samples = np.mean(samples, axis=1)\n",
    "    \n",
    "            # pad so divisible by 3\n",
    "            samples = np.append(samples, np.zeros(3 - (len(samples) % 3)))\n",
    "    \n",
    "            # perform STFT on 10sec samples\n",
    "            clip_samples = np.split(samples, 3, axis=0)\n",
    "    \n",
    "            i=0\n",
    "            for sample in clip_samples:\n",
    "                SFT = signal.ShortTimeFFT.from_window(win_param='tukey', \n",
    "                                                      fs=sample_rate, \n",
    "                                                      nperseg=sample_rate//20,      #make 20Hz minimum sampled frequency\n",
    "                                                      noverlap=(sample_rate//20)//2,  \n",
    "                                                      fft_mode='onesided', \n",
    "                                                      scale_to='magnitude', \n",
    "                                                      phase_shift=None,\n",
    "                                                      symmetric_win=True)\n",
    "                Zxx = SFT.stft(sample)\n",
    "                t = SFT.t(len(sample))\n",
    "                f = SFT.f\n",
    "                np.save(save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*3)+i) + \"_Z.npy\", Zxx)\n",
    "                np.save(save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*3)+i) + \"_T.npy\", t)\n",
    "                np.save(save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*3)+i) + \"_F.npy\", f)\n",
    "                i+=1\n",
    "            n+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9f7e4-cbc0-47ba-abdd-596c267b27fe",
   "metadata": {},
   "source": [
    "### Run CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84ba56b-b14e-4537-8643-80ba5cdfa63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Define the device (use GPU if available)\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "#device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412fc862-0da8-4916-a9fa-2723af72d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.Zsamples = []\n",
    "        self.Tsamples = []\n",
    "        self.Fsamples = []\n",
    "        \n",
    "        # Load file paths and labels\n",
    "        for label, class_dir in enumerate(os.listdir(root_dir)):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\"Z.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Zsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"T.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Tsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"F.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Fsamples.append((file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Zsamples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.Zsamples[idx]\n",
    "        data = np.load(file_path)  # Load numpy array\n",
    "        data = np.expand_dims(data, axis=0)\n",
    "        data = np.abs(data) #take magnitude only\n",
    "        data = torch.tensor(data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def spectrograms(self, idx):\n",
    "        Zfile_path, label = self.Zsamples[idx]\n",
    "        Tfile_path, label = self.Tsamples[idx]\n",
    "        Ffile_path, label = self.Fsamples[idx]\n",
    "        Zdata = np.load(Zfile_path)  # Load numpy array\n",
    "        Tdata = np.load(Tfile_path)  # Load numpy array\n",
    "        Fdata = np.load(Ffile_path)  # Load numpy array\n",
    "        # Create a 2x2 subplot grid\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10, 16))\n",
    "        \n",
    "        # First subplot\n",
    "        c1 = axes[0].pcolormesh(Tdata, Fdata, np.log(np.abs(Zdata)), cmap='gnuplot')\n",
    "        fig.colorbar(c1, ax=axes[0])\n",
    "        axes[0].set_title(\"Spectrogram Magnitude\")\n",
    "        \n",
    "        # Second subplot\n",
    "        c2 = axes[1].pcolormesh(Tdata, Fdata, np.angle(Zdata), cmap='gnuplot')\n",
    "        fig.colorbar(c2, ax=axes[1])\n",
    "        axes[1].set_title(\"Spectrogram Angle\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a75ebe-2975-48fa-baef-6c16f3b9fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "# - ToTensor: Converts the image to a PyTorch tensor\n",
    "# - Normalize: Normalizes using mean and std of the ImageNet dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((552, 400)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c57e6e2b-59e3-4131-852c-eed17e80345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6720\n",
      "1680\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"genres_images/\"\n",
    "dataset = NumpyDataset(root_dir=root_dir, transform=transform)\n",
    "\n",
    "total_count = len(dataset)\n",
    "train_count = int(0.8 * total_count)\n",
    "test_count = total_count - train_count\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_count, test_count))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b147f97b-acce-43c0-890d-6f8c862a954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e9197e4-b78f-4097-815e-9b2d842f3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.spectrograms(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd860fb1-9150-49ac-9a1f-b4c068d6e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.spectrograms(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae132c30-a382-4e7f-b038-af812e3753be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Datasets and DataLoaders for training and testing\n",
    "# Define the root directory where data is stored\n",
    "root_dir = \"genres_images/\"\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print('Data loaders created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "077d0483-7ce9-4f5d-af30-be6ed24432fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicNet(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (conv4): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv2d(16, 20, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (conv6): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=82800, out_features=5000, bias=True)\n",
      "  (fc2): Linear(in_features=5000, out_features=1000, bias=True)\n",
      "  (fc3): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (fc4): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class MusicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MusicNet, self).__init__()\n",
    "        # C1: Convolutional Layer (input channels: 1, output channels: 6, kernel size: 3x3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "        # C2: Convolutional Layer (input channels: 6, output channels: 12, kernel size: 3x3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "        # S3: Subsampling (Max Pooling with kernel size: 2x2 and stride: 2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # C4: Convolutional Layer (input channels: 12, output channels: 24, kernel size: 3x3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=8, out_channels=12, kernel_size=3, stride=1, padding='same')\n",
    "\n",
    "        # C5: Convolutional Layer (input channels: 6, output channels: 16, kernel size: 3x3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "        # S6: Subsampling (Max Pooling with kernel size: 2x2 and stride: 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # C7: Convolutional Layer (input channels: 48, output channels: 96, kernel size: 3x3)\n",
    "        self.conv5 = nn.Conv2d(in_channels=16, out_channels=20, kernel_size=3, stride=1, padding='same')\n",
    "\n",
    "        # C8: Convolutional Layer (input channels: 6, output channels: 16, kernel size: 3x3)\n",
    "        self.conv6 = nn.Conv2d(in_channels=20, out_channels=24, kernel_size=3, stride=1, padding='same')\n",
    "        \n",
    "        # S9: Subsampling (Max Pooling with kernel size: 2x2 and stride: 2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # C10: Fully connected convolutional layer (input size: 192, output size: 120)\n",
    "        self.fc1 = nn.Linear(in_features=24*69*50, out_features=5000)\n",
    "        \n",
    "        # F11: Fully connected layer (input size: 10000, output size: 1000)\n",
    "        self.fc2 = nn.Linear(in_features=5000, out_features=1000)\n",
    "\n",
    "        # F12: Fully connected layer (input size: 1000, output size: 100)\n",
    "        self.fc3 = nn.Linear(in_features=1000, out_features=100)\n",
    "        \n",
    "        # Output layer (input size: 100, output size: 10)\n",
    "        self.fc4 = nn.Linear(in_features=100, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the first convolution and activation function\n",
    "        # print('x dims', x.shape)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))    # C1\n",
    "        x = F.relu(self.conv2(x))    # C2\n",
    "        x = self.pool1(x)            # S3\n",
    "        x = F.relu(self.conv3(x))    # C4\n",
    "        x = F.relu(self.conv4(x))    # C5\n",
    "        x = self.pool2(x)            # S6\n",
    "        x = F.relu(self.conv5(x))    # C7\n",
    "        x = F.relu(self.conv6(x))    # C8\n",
    "        x = self.pool3(x)            # S9\n",
    "        \n",
    "        x = x.view(x.shape[0], 24*69*50)       # Flatten before passing to fully connected layers\n",
    "        # Fully connected layers with activation functions\n",
    "        x = F.relu(self.fc1(x))      # C10\n",
    "        x = F.relu(self.fc2(x))      # F11\n",
    "        x = F.relu(self.fc3(x))      # F12\n",
    "        # Output layer (no activation function because we will use CrossEntropyLoss which includes Softmax)\n",
    "        x = self.fc4(x)              # Output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MusicNet().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081ac506-31be-4c22-8a4e-e7027e72563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer are set up.\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # includes softmax function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print('Model, loss function, and optimizer are set up.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d29389cb-6233-4fed-8e81-7d03473def57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 2.4145\n",
      "Epoch [2/6], Loss: 1.7026\n",
      "Epoch [3/6], Loss: 1.3647\n",
      "Epoch [4/6], Loss: 1.1233\n",
      "Epoch [5/6], Loss: 0.9354\n",
      "Epoch [6/6], Loss: 0.6977\n",
      "elapsed time 5514.4 sec.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    time_start = time()\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Note: we should do early stopping here! \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    time_stop = time()\n",
    "    time_elapsed = time_stop - time_start\n",
    "    print(f'elapsed time {round(time_elapsed,1)} sec.')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ef7f07-bfe4-451d-ac79-42612b5cc5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test dataset: 56.01%\n"
     ]
    }
   ],
   "source": [
    "# Testing loop\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy of the model on the test dataset: {accuracy:.2f}%')\n",
    "\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f1dcc-1881-407b-ad7a-052cd50d149d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
