{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2264c68b-cf78-423a-b664-7d9cbc2258de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2414d1-573a-4d61-b27f-ff52dc091e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b5be5f-4dc0-4de5-92dc-04ee6c987535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=16000, segment_length=5*16000, exclude_files=None, train_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory with genre folders containing audio files.\n",
    "            sample_rate (int): Target sample rate for audio files.\n",
    "            segment_length (int): Length of each audio segment in samples (6 seconds).\n",
    "            set_type (str): Specify \"train\" for training set and \"test\" for testing set.\n",
    "            train_ratio (float): The ratio of the data to use for training (e.g., 0.2 for 20% train, 80% test).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length = segment_length  # Length of each segment (10 seconds)\n",
    "        self.exclude_files = exclude_files if exclude_files else []\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))  # Get genre names as class labels\n",
    "\n",
    "        file_limit = 289\n",
    "        # Collect file paths and corresponding labels\n",
    "        for label_idx, genre in enumerate(self.classes):\n",
    "            genre_folder = os.path.join(root_dir, genre)\n",
    "            i = 0\n",
    "            for file_name in os.listdir(genre_folder):\n",
    "                if i == file_limit:\n",
    "                    break\n",
    "                if file_name.endswith(\".wav\") and file_name not in self.exclude_files:\n",
    "                    self.file_paths.append(os.path.join(genre_folder, file_name))\n",
    "                    self.labels.append(label_idx)\n",
    "                    i += 1\n",
    "\n",
    "        # Shuffle and split data\n",
    "        combined = list(zip(self.file_paths, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.file_paths, self.labels = zip(*combined)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths) * 6  # Each file is split into 6 segments\n",
    "    \n",
    "    def load_audio_with_fallback(self, file_path):\n",
    "        \"\"\"\n",
    "        Load audio file using torchaudio, and fall back to librosa if needed.\n",
    "        \"\"\"\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a 5-second audio segment and its label from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the audio segment to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the Fourier-transformed waveform segment and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Determine file index and segment index\n",
    "        file_idx = idx // 6       # Index of the file in self.file_paths\n",
    "        segment_idx = idx % 6     # Segment within the file (for 5-second clips)\n",
    "\n",
    "        # Load the waveform\n",
    "        audio_path = self.file_paths[file_idx]\n",
    "        waveform = self.load_audio_with_fallback(audio_path)\n",
    "        if waveform is None:\n",
    "            print(f\"Skipping {audio_path} due to load failure.\")\n",
    "            return None\n",
    "\n",
    "        # Ensure mono audio\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Calculate start and end for each 5-second segment\n",
    "        start = segment_idx * self.segment_length\n",
    "        end = start + self.segment_length\n",
    "        waveform_segment = waveform[:, start:end]\n",
    "\n",
    "        # If segment is shorter than segment_length, pad with zeros\n",
    "        num_samples = waveform_segment.size(1)\n",
    "        if num_samples < self.segment_length:\n",
    "            padding = self.segment_length - num_samples\n",
    "            waveform_segment = torch.nn.functional.pad(waveform_segment, (0, padding))\n",
    "\n",
    "        # Perform Fourier Transform on the segment and keep only the first half\n",
    "        waveform_np = waveform_segment.numpy()  # Convert tensor to numpy array\n",
    "        fourier_transform = np.abs(fft(waveform_np[0]))[:self.segment_length // 2]  # Only the first half\n",
    "\n",
    "        # Retrieve label\n",
    "        label = self.labels[file_idx]\n",
    "\n",
    "        return torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0), label  # Add channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbb2433-e400-4015-b439-4545d76c7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another 1D CNN model for Fourier-transformed audio data\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, input_length=5*16000 // 2):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer and pooling\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolutional layer and pooling\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolutional layer and pooling\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fourth convolutional layer and pooling\n",
    "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fifth convolutional layer and pooling\n",
    "        self.conv5 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool5 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flattened size after the last pooling layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_length)  # Batch size 1, 1 channel, input length\n",
    "            out = F.relu(self.conv1(dummy_input))\n",
    "            out = self.pool1(out)\n",
    "            out = F.relu(self.conv2(out))\n",
    "            out = self.pool2(out)\n",
    "            out = F.relu(self.conv3(out))\n",
    "            out = self.pool3(out)\n",
    "            out = F.relu(self.conv4(out))\n",
    "            out = self.pool4(out)\n",
    "            out = F.relu(self.conv5(out))\n",
    "            out = self.pool5(out)\n",
    "            self.flattened_size = out.shape[1] * out.shape[2]  # Channels * Width after conv/pooling layers\n",
    "\n",
    "        # Define fully connected layers using the computed flattened size\n",
    "        self.fc1 = nn.Linear(in_features=self.flattened_size, out_features=3200)\n",
    "        self.fc2 = nn.Linear(in_features=3200, out_features=1600)\n",
    "        self.fc3 = nn.Linear(in_features=1600, out_features=800)\n",
    "        self.fc4 = nn.Linear(in_features=800, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional and pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor to match the input of the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))    # Pass through first fully connected layer\n",
    "        x = F.relu(self.fc2(x))    # Pass through second fully connected layer\n",
    "        x = F.relu(self.fc3(x))    # Pass through third fully connected layer\n",
    "        x = self.fc4(x)            # Output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016d28c6-4fe7-4131-9f83-510cf0c92bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioCNN(\n",
      "  (conv1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=160000, out_features=3200, bias=True)\n",
      "  (fc2): Linear(in_features=3200, out_features=1600, bias=True)\n",
      "  (fc3): Linear(in_features=1600, out_features=800, bias=True)\n",
      "  (fc4): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_path = \"MusicGenreClassifier1D_M4.pth\"\n",
    "model = AudioCNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df6e21b9-dea9-445e-862f-17aa9efabcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop to calculate accuracy\n",
    "def evaluate_single_file(model, file_path, label, classes, device, segment_length=5*16000):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(f\"Evaluating file: {file_path}, True Label: {label}\")\n",
    "\n",
    "    label_index = classes.index(label)\n",
    "    \n",
    "    dataset = AudioDataset(root_dir=\"genres_test/\", segment_length=segment_length)\n",
    "    waveform = dataset.load_audio_with_fallback(file_path)\n",
    "\n",
    "    # Ensure mono and process segments\n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    num_segments = waveform.size(1) // segment_length\n",
    "\n",
    "    # Calculate the number of segments\n",
    "    num_segments = (waveform.size(1) + segment_length - 1) // segment_length  # Round up for last segment\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for segment_idx in range(num_segments):\n",
    "            start = segment_idx * segment_length\n",
    "            end = start + segment_length\n",
    "            waveform_segment = waveform[:, start:end]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if waveform_segment.size(1) < segment_length:\n",
    "                padding = segment_length - waveform_segment.size(1)\n",
    "                waveform_segment = torch.nn.functional.pad(waveform_segment, (0, padding))\n",
    "            \n",
    "            # Perform Fourier Transform\n",
    "            waveform_np = waveform_segment.numpy()\n",
    "            fourier_transform = np.abs(fft(waveform_np[0]))[:segment_length // 2]\n",
    "            \n",
    "            # Convert to tensor\n",
    "            input_tensor = torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            # Check and update correctness\n",
    "            if predicted.item() == label_index:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            # Print prediction for the segment\n",
    "            print(f\"Segment {segment_idx + 1}: Predicted Genre: {classes[predicted.item()]}\")\n",
    "\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be35f2f7-ddb4-449f-b7fd-3dca6c947cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: genres_test/rock/RockOrBust.wav, True Label: rock\n",
      "Segment 1: Predicted Genre: metal\n",
      "Segment 2: Predicted Genre: metal\n",
      "Segment 3: Predicted Genre: rock\n",
      "Segment 4: Predicted Genre: rock\n",
      "Segment 5: Predicted Genre: metal\n",
      "Segment 6: Predicted Genre: metal\n",
      "Segment 7: Predicted Genre: metal\n",
      "Segment 8: Predicted Genre: metal\n",
      "Segment 9: Predicted Genre: metal\n",
      "Segment 10: Predicted Genre: metal\n",
      "Segment 11: Predicted Genre: rock\n",
      "Segment 12: Predicted Genre: rock\n",
      "Segment 13: Predicted Genre: metal\n",
      "Segment 14: Predicted Genre: metal\n",
      "Segment 15: Predicted Genre: rock\n",
      "Segment 16: Predicted Genre: metal\n",
      "Segment 17: Predicted Genre: metal\n",
      "Segment 18: Predicted Genre: metal\n",
      "Segment 19: Predicted Genre: metal\n",
      "Segment 20: Predicted Genre: rock\n",
      "Segment 21: Predicted Genre: rock\n",
      "Segment 22: Predicted Genre: metal\n",
      "Segment 23: Predicted Genre: metal\n",
      "Segment 24: Predicted Genre: metal\n",
      "Segment 25: Predicted Genre: metal\n",
      "Segment 26: Predicted Genre: metal\n",
      "Segment 27: Predicted Genre: metal\n",
      "Segment 28: Predicted Genre: metal\n",
      "Segment 29: Predicted Genre: rock\n",
      "Segment 30: Predicted Genre: rock\n",
      "Segment 31: Predicted Genre: rock\n",
      "Segment 32: Predicted Genre: metal\n",
      "Segment 33: Predicted Genre: metal\n",
      "Segment 34: Predicted Genre: metal\n",
      "Segment 35: Predicted Genre: metal\n",
      "Segment 36: Predicted Genre: pop\n",
      "Segment 37: Predicted Genre: jazz\n",
      "Accuracy: 27.03%\n"
     ]
    }
   ],
   "source": [
    "#chose test song\n",
    "path = \"genres_test/rock/RockOrBust.wav\"\n",
    "label = \"rock\"\n",
    "\n",
    "genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "\n",
    "root_dir = \"genres_test/\"\n",
    "music_dataset = AudioDataset(root_dir=root_dir, exclude_files=[])\n",
    "\n",
    "music_loader = DataLoader(dataset=music_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "evaluate_single_file(model=model,\n",
    "                       file_path=path,\n",
    "                       label=label,\n",
    "                       classes=music_dataset.classes,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b08c49-0157-4dd9-80cb-51d09999d59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9243ea-f694-47bc-9624-6c9feed5c84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
