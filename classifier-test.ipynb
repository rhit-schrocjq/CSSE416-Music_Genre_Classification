{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2264c68b-cf78-423a-b664-7d9cbc2258de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2414d1-573a-4d61-b27f-ff52dc091e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfa381c-0efb-44ee-9918-9199d2a1a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chose test song\n",
    "path = \"genres_test/classical\"\n",
    "name = \"nimrod3\"\n",
    "label = \"classical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b5be5f-4dc0-4de5-92dc-04ee6c987535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_rate=16000, segment_length=5*16000, exclude_files=None, set_type=\"train\", train_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory with genre folders containing audio files.\n",
    "            sample_rate (int): Target sample rate for audio files.\n",
    "            segment_length (int): Length of each audio segment in samples (6 seconds).\n",
    "            set_type (str): Specify \"train\" for training set and \"test\" for testing set.\n",
    "            train_ratio (float): The ratio of the data to use for training (e.g., 0.2 for 20% train, 80% test).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length = segment_length  # Length of each segment (10 seconds)\n",
    "        self.exclude_files = exclude_files if exclude_files else []\n",
    "        self.set_type = set_type\n",
    "        self.train_ratio = train_ratio\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = sorted(os.listdir(root_dir))  # Get genre names as class labels\n",
    "\n",
    "        file_limit = 20\n",
    "        # Collect file paths and corresponding labels\n",
    "        for label_idx, genre in enumerate(self.classes):\n",
    "            genre_folder = os.path.join(root_dir, genre)\n",
    "            i = 0\n",
    "            for file_name in os.listdir(genre_folder):\n",
    "                if i == file_limit:\n",
    "                    break\n",
    "                if file_name.endswith(\".wav\") and file_name not in self.exclude_files:\n",
    "                    self.file_paths.append(os.path.join(genre_folder, file_name))\n",
    "                    self.labels.append(label_idx)\n",
    "                    i += 1\n",
    "\n",
    "        # Shuffle and split data\n",
    "        combined = list(zip(self.file_paths, self.labels))\n",
    "        random.shuffle(combined)\n",
    "        self.file_paths, self.labels = zip(*combined)\n",
    "\n",
    "        # Perform train-test split\n",
    "        split_index = int(len(self.file_paths) * self.train_ratio)\n",
    "        if self.set_type == \"train\":\n",
    "            # Use the first portion as the training set\n",
    "            self.file_paths = self.file_paths[:split_index]\n",
    "            self.labels = self.labels[:split_index]\n",
    "        else:\n",
    "            # Use the remaining portion as the testing set\n",
    "            self.file_paths = self.file_paths[split_index:]\n",
    "            self.labels = self.labels[split_index:]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths) * 6  # Each file is split into 6 segments\n",
    "    \n",
    "    def load_audio_with_fallback(self, file_path):\n",
    "        \"\"\"\n",
    "        Load audio file using torchaudio, and fall back to librosa if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "            if sr != self.sample_rate:\n",
    "                waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"torchaudio failed for {file_path} with error: {e}. Trying librosa as fallback.\")\n",
    "            waveform, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "            waveform = torch.tensor(waveform).unsqueeze(0)  # Convert to PyTorch format, add channel dim\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a 5-second audio segment and its label from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the audio segment to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the Fourier-transformed waveform segment and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Determine file index and segment index\n",
    "        file_idx = idx // 6       # Index of the file in self.file_paths\n",
    "        segment_idx = idx % 6     # Segment within the file (for 5-second clips)\n",
    "\n",
    "        # Load the waveform\n",
    "        audio_path = self.file_paths[file_idx]\n",
    "        waveform = self.load_audio_with_fallback(audio_path)\n",
    "        if waveform is None:\n",
    "            print(f\"Skipping {audio_path} due to load failure.\")\n",
    "            return None\n",
    "\n",
    "        # Ensure mono audio\n",
    "        if waveform.size(0) > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Calculate start and end for each 5-second segment\n",
    "        start = segment_idx * self.segment_length\n",
    "        end = start + self.segment_length\n",
    "        waveform_segment = waveform[:, start:end]\n",
    "\n",
    "        # If segment is shorter than segment_length, pad with zeros\n",
    "        num_samples = waveform_segment.size(1)\n",
    "        if num_samples < self.segment_length:\n",
    "            padding = self.segment_length - num_samples\n",
    "            waveform_segment = torch.nn.functional.pad(waveform_segment, (0, padding))\n",
    "\n",
    "        # Perform Fourier Transform on the segment and keep only the first half\n",
    "        waveform_np = waveform_segment.numpy()  # Convert tensor to numpy array\n",
    "        fourier_transform = np.abs(fft(waveform_np[0]))[:self.segment_length // 2]  # Only the first half\n",
    "\n",
    "        # Retrieve label\n",
    "        label = self.labels[file_idx]\n",
    "\n",
    "        return torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0), label  # Add channel dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbb2433-e400-4015-b439-4545d76c7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define another 1D CNN model for Fourier-transformed audio data\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, input_length=5*16000 // 2):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer and pooling\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolutional layer and pooling\n",
    "        self.conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolutional layer and pooling\n",
    "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fourth convolutional layer and pooling\n",
    "        self.conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fifth convolutional layer and pooling\n",
    "        self.conv5 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n",
    "        self.pool5 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the flattened size after the last pooling layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, input_length)  # Batch size 1, 1 channel, input length\n",
    "            out = F.relu(self.conv1(dummy_input))\n",
    "            out = self.pool1(out)\n",
    "            out = F.relu(self.conv2(out))\n",
    "            out = self.pool2(out)\n",
    "            out = F.relu(self.conv3(out))\n",
    "            out = self.pool3(out)\n",
    "            out = F.relu(self.conv4(out))\n",
    "            out = self.pool4(out)\n",
    "            out = F.relu(self.conv5(out))\n",
    "            out = self.pool5(out)\n",
    "            self.flattened_size = out.shape[1] * out.shape[2]  # Channels * Width after conv/pooling layers\n",
    "\n",
    "        # Define fully connected layers using the computed flattened size\n",
    "        self.fc1 = nn.Linear(in_features=self.flattened_size, out_features=4800)\n",
    "        self.fc2 = nn.Linear(in_features=4800, out_features=2400)\n",
    "        self.fc3 = nn.Linear(in_features=2400, out_features=800)\n",
    "        self.fc4 = nn.Linear(in_features=800, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through convolutional and pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor to match the input of the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))    # Pass through first fully connected layer\n",
    "        x = F.relu(self.fc2(x))    # Pass through second fully connected layer\n",
    "        x = F.relu(self.fc3(x))    # Pass through third fully connected layer\n",
    "        x = self.fc4(x)            # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016d28c6-4fe7-4131-9f83-510cf0c92bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioCNN(\n",
      "  (conv1): Conv1d(1, 8, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=160000, out_features=4800, bias=True)\n",
      "  (fc2): Linear(in_features=4800, out_features=2400, bias=True)\n",
      "  (fc3): Linear(in_features=2400, out_features=800, bias=True)\n",
      "  (fc4): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_path = \"MusicGenreClassifier1D_M4.pth\"\n",
    "model = AudioCNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d180cea3-fb91-455f-8b62-a2f86644d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test song\n",
    "def process_audio_file(file_path, label, sample_rate=16000, segment_length=5*16000):\n",
    "    \"\"\"\n",
    "    Process a single audio file: split it into 5-second segments and apply Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "        label (int): Label corresponding to the genre/class of the audio file.\n",
    "        sample_rate (int): Target sample rate for audio.\n",
    "        segment_length (int): Length of each audio segment in samples (5 seconds).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains Fourier-transformed segment and label.\n",
    "    \"\"\"\n",
    "    def load_audio_with_fallback(file_path):\n",
    "        \"\"\"Load audio file with torchaudio\"\"\"\n",
    "        waveform, sr = torchaudio.load(file_path)\n",
    "        if sr != sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)(waveform)\n",
    "        return waveform\n",
    "\n",
    "    # Load audio\n",
    "    waveform = load_audio_with_fallback(file_path)\n",
    "    if waveform is None:\n",
    "        print(f\"Skipping {file_path} due to load failure.\")\n",
    "        return []\n",
    "\n",
    "    # Ensure mono audio\n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Split into 5-second segments\n",
    "    num_samples = waveform.size(1)\n",
    "    segments = []\n",
    "    for start in range(0, num_samples, segment_length):\n",
    "        end = start + segment_length\n",
    "        waveform_segment = waveform[:, start:end]\n",
    "\n",
    "        # If segment is shorter than segment_length, pad with zeros\n",
    "        if waveform_segment.size(1) < segment_length:\n",
    "            padding = segment_length - waveform_segment.size(1)\n",
    "            waveform_segment = torch.nn.functional.pad(waveform_segment, (0, padding))\n",
    "\n",
    "        # Perform Fourier Transform and keep only the first half\n",
    "        waveform_np = waveform_segment.numpy()  # Convert tensor to numpy array\n",
    "        fourier_transform = np.abs(fft(waveform_np[0]))[:segment_length // 2]\n",
    "\n",
    "        # Append the transformed data and label\n",
    "        segments.append((torch.tensor(fourier_transform, dtype=torch.float32).unsqueeze(0), label))  # Add channel dim\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bddafae2-81a5-4d16-a72b-a27cf92af82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_song = process_audio_file(path + \"/\" + name + \".wav\", label)\n",
    "genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6e21b9-dea9-445e-862f-17aa9efabcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop to calculate accuracy\n",
    "def music_classifier(model, test_loader, classes, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(\"Real Genre : Guess\")\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for waveforms, labels in test_loader:\n",
    "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(waveforms)\n",
    "\n",
    "            # Get the predicted class (index of max log-probability)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Print real and predicted genre for each sample in the batch\n",
    "            for label, pred in zip(labels, predicted):\n",
    "                print(f\"{classes[label.item()]:10} : {classes[pred.item()]:10}\")\n",
    "                \n",
    "            # Update total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy\n",
    "    print(f'Test Accuracy of the model on the example song: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be35f2f7-ddb4-449f-b7fd-3dca6c947cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Genre : Guess\n",
      "classical  : classical \n",
      "classical  : classical \n",
      "classical  : classical \n",
      "classical  : classical \n",
      "classical  : classical \n",
      "classical  : jazz      \n",
      "Test Accuracy of the model on the example song: 83.33%\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"genres_test/\"\n",
    "music_dataset = AudioDataset(root_dir=root_dir, set_type=\"test\", exclude_files=[])\n",
    "\n",
    "music_loader = DataLoader(dataset=music_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "music_classifier(model, music_loader, music_dataset.classes, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
