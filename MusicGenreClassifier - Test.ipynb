{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edadeff0-d6e4-4f83-ac3f-735bf27a7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73989c08-570c-4570-82a5-f76451dd22f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "#device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884dd3cd-b075-46a8-af0f-23447594ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chose test song\n",
    "path = \"genres_user_test/hiphop\"\n",
    "name = \"PYN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f21673-9e5c-477f-9736-84be1f461b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.Zsamples = []\n",
    "        self.Tsamples = []\n",
    "        self.Fsamples = []\n",
    "        \n",
    "        # Load file paths and labels\n",
    "        for label, class_dir in enumerate(os.listdir(root_dir)):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                if (class_path != path):\n",
    "                    continue\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\"Z.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Zsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"T.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Tsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"F.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Fsamples.append((file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Zsamples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.Zsamples[idx]\n",
    "        data = np.load(file_path)  # Load numpy array\n",
    "        real = np.real(data)\n",
    "        imag = np.imag(data)\n",
    "        mag = np.abs(data)\n",
    "        angle = np.angle(data)\n",
    "        real = np.expand_dims(real, axis=0)\n",
    "        imag = np.expand_dims(imag, axis=0)\n",
    "        mag = np.expand_dims(mag, axis=0)\n",
    "        angle = np.expand_dims(angle, axis=0)\n",
    "        data = np.concatenate((real, imag, mag, angle), axis=0)\n",
    "        data = torch.tensor(data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def spectrograms(self, idx):\n",
    "        Zfile_path, label = self.Zsamples[idx]\n",
    "        Tfile_path, label = self.Tsamples[idx]\n",
    "        Ffile_path, label = self.Fsamples[idx]\n",
    "        Zdata = np.load(Zfile_path)  # Load numpy array\n",
    "        Tdata = np.load(Tfile_path)  # Load numpy array\n",
    "        Fdata = np.load(Ffile_path)  # Load numpy array\n",
    "        print(Zdata.shape)\n",
    "        # Create a 2x2 subplot grid\n",
    "        fig, axes = plt.subplots(5, 1, figsize=(10, 16))\n",
    "        \n",
    "        # First subplot\n",
    "        c1 = axes[0].pcolormesh(Tdata, Fdata, np.log(np.abs(Zdata)), cmap='gnuplot')\n",
    "        fig.colorbar(c1, ax=axes[0])\n",
    "        axes[0].set_title(\"Spectrogram Magnitude\")\n",
    "        \n",
    "        # Second subplot\n",
    "        c2 = axes[1].pcolormesh(Tdata, Fdata, np.angle(Zdata), cmap='gnuplot')\n",
    "        fig.colorbar(c2, ax=axes[1])\n",
    "        axes[1].set_title(\"Spectrogram Angle\")      \n",
    "\n",
    "        # Third subplot\n",
    "        c3 = axes[2].pcolormesh(Tdata, Fdata, np.log(np.square(np.real(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c3, ax=axes[2])\n",
    "        axes[2].set_title(\"Spectrogram Real\")  \n",
    "\n",
    "        # Fourth subplot\n",
    "        c4 = axes[3].pcolormesh(Tdata, Fdata, np.log(np.square(np.imag(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c4, ax=axes[3])\n",
    "        axes[3].set_title(\"Spectrogram Imag\")  \n",
    "\n",
    "        # Fifth subplot\n",
    "        c5 = axes[4].pcolormesh(Tdata, Fdata, np.log(np.square(np.imag(Zdata)) + np.square(np.real(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c4, ax=axes[4])\n",
    "        axes[4].set_title(\"Spectrogram Imag + Real\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36bda9eb-33e9-4ca4-b70b-95a9a7450d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "# - ToTensor: Converts the image to a PyTorch tensor\n",
    "# - Normalize: Normalizes using mean and std of the ImageNet dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((544, 240)),\n",
    "    transforms.Normalize((0,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62532600-7efa-4775-a0be-c04fc2667f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicNet(\n",
      "  (cLayer1): Sequential(\n",
      "    (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=4)\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fLayer): Sequential(\n",
      "    (0): Linear(in_features=261120, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class MusicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MusicNet, self).__init__()\n",
    "        self.cLayer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=64, kernel_size=3, stride=1, padding='same', groups=4),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.cLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fLayer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*34*15, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        # Apply the first convolution and activation function\n",
    "        # print('x dims', x.shape)\n",
    "        \n",
    "        x = self.cLayer1(x)\n",
    "        x = self.cLayer2(x)\n",
    "        x = self.cLayer3(x)\n",
    "        x = self.cLayer4(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], 512*34*15)       # Flatten before passing to fully connected layers\n",
    "        \n",
    "        if not return_features:\n",
    "            x = self.fLayer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MusicNet().to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac8fb4e-b9c0-4dcc-b8f3-49d3b25541af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"MusicGenreClassifier.pth\"\n",
    "model = MusicNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768bc4fd-7c5c-4e75-a103-8b8b41c6ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before loading music, make sure it is at least 6 seconds long and sampled at 22050 Hz\n",
    "# ffmpeg -i foo.mp3 -vn -acodec pcm_s16le -ac 1 -ar 22050 -f wav foo.wav\n",
    "num_divisions = 5\n",
    "clip_length = 132301\n",
    "\n",
    "sample_rate, samples = wav.read(path + \"/\" + name + \".wav\") # extract audio\n",
    "        \n",
    "# convert to mono\n",
    "if len(samples.shape) > 1:\n",
    "    # Do a mean of all channels and keep it in one channel\n",
    "    samples = np.mean(samples, axis=1)\n",
    "\n",
    "# pad so divisible by num_divisions\n",
    "samples = np.append(samples, np.zeros(num_divisions - (len(samples) % num_divisions)))\n",
    "\n",
    "# perform STFT on 6sec samples\n",
    "clip_samples=[]\n",
    "start = 0\n",
    "end = start + clip_length\n",
    "\n",
    "while (end < len(samples)):\n",
    "    clip_samples.append(samples[start: end])\n",
    "    start = end\n",
    "    end = end + clip_length\n",
    "\n",
    "i=0\n",
    "for sample in clip_samples:\n",
    "    SFT = signal.ShortTimeFFT.from_window(win_param='tukey', \n",
    "                                          fs=sample_rate, \n",
    "                                          nperseg=sample_rate//20,      #make 20Hz minimum sampled frequency\n",
    "                                          noverlap=(sample_rate//20)//2,  #50% overlap\n",
    "                                          fft_mode='onesided', \n",
    "                                          scale_to='magnitude', \n",
    "                                          phase_shift=None,\n",
    "                                          symmetric_win=True)\n",
    "    Zxx = SFT.stft(sample)\n",
    "    t = SFT.t(len(sample))\n",
    "    f = SFT.f\n",
    "    np.save(path + \"/\" + name + \"_\" + str(i) + \"_Z.npy\", Zxx)\n",
    "    np.save(path + \"/\" + name + \"_\" + str(i) + \"_T.npy\", t)\n",
    "    np.save(path + \"/\" + name + \"_\" + str(i) + \"_F.npy\", f)\n",
    "    i+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6259b31-70c8-45e1-a660-68f6a9dfb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e136b4-271e-4a7f-9f09-cb7f61e27041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "def music_classifier(model, music_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(\"Real Genre : Guess\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in music_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Print the real and predicted genre for each sample in the batch\n",
    "            for label, pred in zip(labels, predicted):\n",
    "                print(f\"{genres[label]:10} : {genres[pred]:10}\")\n",
    "                \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy of the model on the test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38953b2e-d345-40fa-bf2c-6009ff408081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "Real Genre : Guess\n",
      "hiphop     : classical \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : reggae    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "hiphop     : hiphop    \n",
      "Test Accuracy of the model on the test dataset: 95.65%\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"genres_user_test/\"\n",
    "music_dataset = NumpyDataset(root_dir=root_dir, transform=transform)\n",
    "\n",
    "print(len(music_dataset))\n",
    "\n",
    "music_loader = DataLoader(dataset=music_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "music_classifier(model, music_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b7553-878f-4d36-ba8b-f11442f1ea60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
