{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25639d23-11cc-4d26-a91b-fba5ecf3e11a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as signal\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from panns_inference import AudioTagging\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d297645-a6a9-4c65-956d-cc3101b53ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"genres_original\"\n",
    "genres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"]\n",
    "train_save_path = \"train_genres_images\"\n",
    "test_save_path = \"test_genres_images\"\n",
    "num_divisions = 5\n",
    "max_per_genre = 350\n",
    "clip_length = 0\n",
    "train_per_test = 350 #really dont need the test set here, will split later\n",
    "\n",
    "if (False):\n",
    "    for genre in genres:\n",
    "        files = [f for f in os.listdir(path + \"/\" + genre) if os.path.isfile(os.path.join(path + \"/\" + genre, f))]\n",
    "        print(\"Generating \" + genre)\n",
    "        os.makedirs(os.path.dirname(train_save_path + \"/\" + genre + \"/\"), exist_ok=True) # make path if doesn't exist\n",
    "        os.makedirs(os.path.dirname(test_save_path + \"/\" + genre + \"/\"), exist_ok=True) # make path if doesn't exist\n",
    "        \n",
    "        n=0\n",
    "        for file in files:\n",
    "            if(n>=max_per_genre):\n",
    "                break\n",
    "                \n",
    "            if(file == \"jazz.00054.wav\"):\n",
    "                continue\n",
    "            else:\n",
    "                sample_rate, samples = wav.read(path + \"/\" + genre + \"/\" + file) # extract audio)\n",
    "                y, sr = librosa.load(path + \"/\" + genre + \"/\" + file, sr=32000, mono=True) #addition for instrument detection----------------------\n",
    "                    \n",
    "            # convert to mono\n",
    "            if len(samples.shape) > 1:\n",
    "                # Do a mean of all channels and keep it in one channel\n",
    "                samples = np.mean(samples, axis=1)\n",
    "                \n",
    "    \n",
    "            # pad so divisible by num_divisions\n",
    "            samples = np.append(samples, np.zeros(num_divisions - (len(samples) % num_divisions)))\n",
    "    \n",
    "            # perform STFT on 6sec samples\n",
    "            clip_samples = np.split(samples, num_divisions, axis=0)\n",
    "            clip_length = len(clip_samples[0])\n",
    "            start = clip_length // 2\n",
    "            end = start + clip_length\n",
    "            \n",
    "            while (end < len(samples)):\n",
    "                clip_samples.append(samples[start: end])\n",
    "                start = end\n",
    "                end = end + clip_length\n",
    "\n",
    "            #addition for instrument detection---------------------------------------------------------------------------------------\n",
    "            # pad so divisible by num_divisions\n",
    "            y = np.append(y, np.zeros(num_divisions - (len(y) % num_divisions)))\n",
    "            \n",
    "            instrument_clip_samples = np.split(y, num_divisions, axis=0)\n",
    "            instrument_clip_length = len(instrument_clip_samples[0])\n",
    "            start = instrument_clip_length // 2\n",
    "            end = start + instrument_clip_length\n",
    "            \n",
    "            while (end < len(y)):\n",
    "                instrument_clip_samples.append(y[start: end])\n",
    "                start = end\n",
    "                end = end + instrument_clip_length\n",
    "    \n",
    "            i=0\n",
    "            for sample in clip_samples:\n",
    "                SFT = signal.ShortTimeFFT.from_window(win_param='tukey', \n",
    "                                                      fs=sample_rate, \n",
    "                                                      nperseg=sample_rate//20,      #make 20Hz minimum sampled frequency\n",
    "                                                      noverlap=(sample_rate//20)//2,  #50% overlap\n",
    "                                                      fft_mode='onesided', \n",
    "                                                      scale_to='magnitude', \n",
    "                                                      phase_shift=None,\n",
    "                                                      symmetric_win=True)\n",
    "                Zxx = SFT.stft(sample)\n",
    "                t = SFT.t(len(sample))\n",
    "                f = SFT.f\n",
    "                Ix = instrument_clip_samples[i]\n",
    "                \n",
    "                if(n%train_per_test == 0):\n",
    "                    np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_I.npy\", Ix) #addition for instrument detection\n",
    "                    np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_Z.npy\", Zxx)\n",
    "                    np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_T.npy\", t)\n",
    "                    np.save(test_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_F.npy\", f)\n",
    "                else:\n",
    "                    np.save(train_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_I.npy\", Ix) #addition for instrument detection\n",
    "                    np.save(train_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_Z.npy\", Zxx)\n",
    "                    np.save(train_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_T.npy\", t)\n",
    "                    np.save(train_save_path + \"/\" + genre + \"/\" + genre + \"_\" + str((n*len(clip_samples))+i) + \"_F.npy\", f)\n",
    "                    \n",
    "                i+=1\n",
    "            n+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6151b60c-7c55-400a-86b8-36d5aa73e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "#device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf52fcc-2ed3-402c-8af2-de4ae0c87734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.Zsamples = []\n",
    "        self.Tsamples = []\n",
    "        self.Fsamples = []\n",
    "        \n",
    "        # Load file paths and labels\n",
    "        for label, class_dir in enumerate(os.listdir(root_dir)):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\"Z.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Zsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"T.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Tsamples.append((file_path, label))\n",
    "                    elif file_name.endswith(\"F.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Fsamples.append((file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Zsamples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.Zsamples[idx]\n",
    "        data = np.load(file_path)  # Load numpy array\n",
    "        real = np.real(data)\n",
    "        imag = np.imag(data)\n",
    "        mag = np.abs(data)\n",
    "        angle = np.angle(data)\n",
    "        real = np.expand_dims(real, axis=0)\n",
    "        imag = np.expand_dims(imag, axis=0)\n",
    "        mag = np.expand_dims(mag, axis=0)\n",
    "        angle = np.expand_dims(angle, axis=0)\n",
    "        data = np.concatenate((real, imag, mag, angle), axis=0)\n",
    "        data = torch.tensor(data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def spectrograms(self, idx):\n",
    "        Zfile_path, label = self.Zsamples[idx]\n",
    "        Tfile_path, label = self.Tsamples[idx]\n",
    "        Ffile_path, label = self.Fsamples[idx]\n",
    "        Zdata = np.load(Zfile_path)  # Load numpy array\n",
    "        Tdata = np.load(Tfile_path)  # Load numpy array\n",
    "        Fdata = np.load(Ffile_path)  # Load numpy array\n",
    "        print(Zdata.shape)\n",
    "        # Create a 2x2 subplot grid\n",
    "        fig, axes = plt.subplots(5, 1, figsize=(10, 16))\n",
    "        \n",
    "        # First subplot\n",
    "        c1 = axes[0].pcolormesh(Tdata, Fdata, np.log(np.abs(Zdata)), cmap='gnuplot')\n",
    "        fig.colorbar(c1, ax=axes[0])\n",
    "        axes[0].set_title(\"Spectrogram Magnitude\")\n",
    "        \n",
    "        # Second subplot\n",
    "        c2 = axes[1].pcolormesh(Tdata, Fdata, np.angle(Zdata), cmap='gnuplot')\n",
    "        fig.colorbar(c2, ax=axes[1])\n",
    "        axes[1].set_title(\"Spectrogram Angle\")      \n",
    "\n",
    "        # Third subplot\n",
    "        c3 = axes[2].pcolormesh(Tdata, Fdata, np.log(np.square(np.real(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c3, ax=axes[2])\n",
    "        axes[2].set_title(\"Spectrogram Real\")  \n",
    "\n",
    "        # Fourth subplot\n",
    "        c4 = axes[3].pcolormesh(Tdata, Fdata, np.log(np.square(np.imag(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c4, ax=axes[3])\n",
    "        axes[3].set_title(\"Spectrogram Imag\")  \n",
    "\n",
    "        # Fifth subplot\n",
    "        c5 = axes[4].pcolormesh(Tdata, Fdata, np.log(np.square(np.imag(Zdata)) + np.square(np.real(Zdata))), cmap='gnuplot')\n",
    "        fig.colorbar(c4, ax=axes[4])\n",
    "        axes[4].set_title(\"Spectrogram Imag + Real\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7787eaca-6e39-4394-8b86-56d9f2740a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "# - ToTensor: Converts the image to a PyTorch tensor\n",
    "# - Normalize: Normalizes using mean and std of the ImageNet dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop((544, 240)),\n",
    "    transforms.Normalize((0,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022b9ff1-b041-4a15-ae9b-b97bbeb3b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24120\n",
      "2691\n"
     ]
    }
   ],
   "source": [
    "train_root_dir = \"train_genres_images/\"\n",
    "test_root_dir = \"test_genres_images/\"\n",
    "\n",
    "train_dataset = NumpyDataset(root_dir=train_root_dir, transform=transform)\n",
    "test_dataset = NumpyDataset(root_dir=test_root_dir, transform=transform)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe07567-bb56-48f7-85ca-e9e1fbef4c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Datasets and DataLoaders for training and testing\n",
    "# Define the root directory where data is stored\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print('Data loaders created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ecd13a-2e0d-43ce-a9b1-916d9a8c2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MusicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MusicNet, self).__init__()\n",
    "\n",
    "        self.cLayer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=64, kernel_size=3, stride=1, padding='same', groups=4),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.cLayer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding='same'),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fLayer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*34*15, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = self.cLayer1(x)\n",
    "        x = self.cLayer2(x)\n",
    "        x = self.cLayer3(x)\n",
    "        x = self.cLayer4(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], 512*34*15)       # Flatten before passing to fully connected layers\n",
    "\n",
    "        if not return_features:\n",
    "            x = self.fLayer(x)\n",
    "             \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1efe121-6297-4d50-b4da-3d7c594eda30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicNet(\n",
      "  (cLayer1): Sequential(\n",
      "    (0): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=4)\n",
      "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cLayer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fLayer): Sequential(\n",
      "    (0): Linear(in_features=261120, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_path = \"MusicGenreClassifier.pth\"\n",
    "model = MusicNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "# Print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a393f10-1aa2-45f9-8057-a346ad05a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (False):\n",
    "    # Dictionary to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Extract features\n",
    "    with torch.no_grad():  # We don't need gradients for this task\n",
    "        for images, label in train_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Extract features from the images\n",
    "            # The VGG16 model outputs feature maps, which we can treat as image features\n",
    "            outputs = model(images, return_features=True)\n",
    "            \n",
    "            # Flatten the outputs to a 1D vector\n",
    "            outputs = outputs.view(outputs.size(0), -1)\n",
    "            \n",
    "            # Append features and labels to lists\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.append(label.cpu().numpy())\n",
    "            \n",
    "    print(\"Model Done\")\n",
    "    \n",
    "    # Convert the lists to arrays\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    # Save the extracted features and labels\n",
    "    # np.savez is used to save multiple arrays into a single file\n",
    "    np.savez('music_features.npz', features=features, labels=labels)\n",
    "    \n",
    "    print(\"Features and labels saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd34911-09ca-4c1f-a6a2-36f8a39f9e4e",
   "metadata": {},
   "source": [
    "# BEGIN INSTRUMENT feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7434f8-401b-47a2-97f2-4b1c5ca49647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Checkpoint path: C:\\Users\\Joshu/panns_data/Cnn14_mAP=0.431.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshu\\anaconda3\\envs\\DeepLearning\\Lib\\site-packages\\panns_inference\\inference.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU number: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize AudioSet classifier\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "audio_tagging = AudioTagging(checkpoint_path=None, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84c566fc-6fb0-4c93-9fd5-fb18d43ea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class InstrumentDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.Isamples = []\n",
    "        \n",
    "        # Load file paths and labels\n",
    "        for label, class_dir in enumerate(os.listdir(root_dir)):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith(\"I.npy\"):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        self.Isamples.append((file_path, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Isamples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.Isamples[idx]\n",
    "        data = np.load(file_path)  # Load numpy array\n",
    "        data = torch.tensor(data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2dcf31-0617-45bb-bc54-219b5b54f393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24120\n",
      "2691\n"
     ]
    }
   ],
   "source": [
    "train_root_dir = \"train_genres_images/\"\n",
    "test_root_dir = \"test_genres_images/\"\n",
    "\n",
    "train_dataset = InstrumentDataset(root_dir=train_root_dir)\n",
    "test_dataset = InstrumentDataset(root_dir=test_root_dir)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5428f7d3-3a9f-4610-b4ae-c98076e94b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e7e4853-43e7-4b18-ac41-b06709653ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (False):\n",
    "    # Dictionary to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Extract features\n",
    "    for audio, label in train_loader:\n",
    "       \n",
    "        # Extract features for the batch\n",
    "        _, embedding = audio_tagging.inference(audio)\n",
    "    \n",
    "        # Flatten the outputs to a 1D vector\n",
    "        outputs = np.array(embedding.flatten())\n",
    "        \n",
    "        # Append features and labels to lists\n",
    "        features.append(outputs)\n",
    "        labels.append(label)\n",
    "    \n",
    "    print(\"Model Done\")\n",
    "    \n",
    "    # Convert the lists to arrays\n",
    "    #features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    # Save the extracted features and labels\n",
    "    # np.savez is used to save multiple arrays into a single file\n",
    "    np.savez('instrument_features.npz', features=features, labels=labels)\n",
    "    \n",
    "    print(\"Features and labels saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aea5e1-b576-40e1-95cd-81b33c00d243",
   "metadata": {},
   "source": [
    "# Train Model with feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11aae490-9c53-4f39-953c-c7b0444e53a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ccad7f0-aaae-4fd7-9293-14aac73728b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24120, 263168)\n",
      "(24120,)\n"
     ]
    }
   ],
   "source": [
    "music_data = np.load('music_features.npz')\n",
    "instrument_data = np.load('instrument_features.npz')\n",
    "features=[]\n",
    "labels=[]\n",
    "\n",
    "if (np.array_equal(music_data['labels'], instrument_data['labels'])):\n",
    "    features = np.concatenate((music_data['features'], instrument_data['features']), axis=1)\n",
    "    labels = music_data['labels'].astype(np.float32)\n",
    "else:\n",
    "    print(\"Missmatched features\")\n",
    "\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d011873-91ac-42b5-8a93-51bc9c39ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, Y, Y_test = train_test_split(features, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8ff6ad6-7037-44ac-9e9b-4e787bde2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[]\n",
    "labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d18c8dd5-5797-488d-bd55-79fad5fde112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, X ,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.X[idx]\n",
    "        label = int(self.Y[idx])\n",
    "        data = torch.tensor(data, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52d77bec-d536-454d-aa83-1a384c8528bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21708\n",
      "2412\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SimpleDataset(X, Y)\n",
    "test_dataset = SimpleDataset(X_test, Y_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee8b3bb0-c5b0-4bd3-b709-261f2d11f3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print('Data loaders created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85cabcfd-5bb0-43ad-b576-abeda4480a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class FFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNet, self).__init__()\n",
    "\n",
    "        self.fLayer = nn.Sequential(\n",
    "            nn.Linear(in_features=263168, out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fLayer(x)   \n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = FFNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f804512-0bd6-4a28-b4e3-7fee2af72c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss function, and optimizer are set up.\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # includes softmax function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "print('Model, loss function, and optimizer are set up.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5db05b95-8dc6-46b2-bc09-d5e0621cdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df9b04d7-3e48-4dd5-9b63-4c1891fcb2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 1.4054', Validation Loss: 1.1952, Validation Accuracy: 59.41%\n",
      "Epoch [2/100], Training Loss: 0.9917', Validation Loss: 1.2357, Validation Accuracy: 60.99%\n",
      "Epoch [3/100], Training Loss: 0.8329', Validation Loss: 1.1264, Validation Accuracy: 61.36%\n",
      "Epoch [4/100], Training Loss: 0.7246', Validation Loss: 0.8159, Validation Accuracy: 71.93%\n",
      "Epoch [5/100], Training Loss: 0.6384', Validation Loss: 0.8894, Validation Accuracy: 68.99%\n",
      "Epoch [6/100], Training Loss: 0.5609', Validation Loss: 0.8208, Validation Accuracy: 71.93%\n",
      "Epoch [7/100], Training Loss: 0.4987', Validation Loss: 0.8354, Validation Accuracy: 71.97%\n",
      "Epoch [8/100], Training Loss: 0.4583', Validation Loss: 0.8206, Validation Accuracy: 71.02%\n",
      "Epoch [9/100], Training Loss: 0.4092', Validation Loss: 0.6969, Validation Accuracy: 75.12%\n",
      "Epoch [10/100], Training Loss: 0.3539', Validation Loss: 0.7844, Validation Accuracy: 74.30%\n",
      "Epoch [11/100], Training Loss: 0.3158', Validation Loss: 0.6619, Validation Accuracy: 76.70%\n",
      "Epoch [12/100], Training Loss: 0.2918', Validation Loss: 0.6461, Validation Accuracy: 76.53%\n",
      "Epoch [13/100], Training Loss: 0.2587', Validation Loss: 0.7798, Validation Accuracy: 72.76%\n",
      "Epoch [14/100], Training Loss: 0.2289', Validation Loss: 0.6612, Validation Accuracy: 76.74%\n",
      "Epoch [15/100], Training Loss: 0.2106', Validation Loss: 0.6848, Validation Accuracy: 77.57%\n",
      "Epoch [16/100], Training Loss: 0.1821', Validation Loss: 0.7238, Validation Accuracy: 77.57%\n",
      "Epoch [17/100], Training Loss: 0.1768', Validation Loss: 0.8796, Validation Accuracy: 74.38%\n",
      "Epoch [18/100], Training Loss: 0.1531', Validation Loss: 0.8058, Validation Accuracy: 76.41%\n",
      "Early stopping triggered\n",
      "elapsed time 610.6 sec.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=5, patience=5):\n",
    "    time_start = time()\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() \n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss /= len(test_loader)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {avg_loss:.4f}', Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "\n",
    "    time_stop = time()\n",
    "    time_elapsed = time_stop - time_start\n",
    "    print(f'elapsed time {round(time_elapsed,1)} sec.')\n",
    "\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=100, patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44cb61be-d66b-48f6-ba8d-2da811a1a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"MixedMusicGenreClassifier.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02571e-e8f3-4c2d-b038-83e98b17ad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
